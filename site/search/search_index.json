{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#code.bpf.BestParameterFinder","title":"<code>BestParameterFinder</code>","text":"Source code in <code>code\\bpf.py</code> <pre><code>class BestParameterFinder:\n\tdef __init__(self, metric: Optional[Callable[[\"BestParameterFinder\", np.ndarray], float]] = None):\n\t\t\"\"\"\n\t\tInitializes the BestParameterFinder.\n\n\t\tArgs:\n\t\t\tmetric (Optional[Callable[[BestParameterFinder, np.ndarray], float]]):\n\t\t\t\tA custom metric function. Defaults to `expWithStd`.\n\t\t\"\"\"\n\t\tself.metric = metric or self.expWithStd\n\t\tself.p_g: Optional[float] = None\n\t\tself.c: Optional[float] = None\n\n\tdef nInfUniform(self, voltages: np.ndarray) -&gt; float:\n\t\t\"\"\"\n\t\tComputes the infinity-norm distance between voltages and a uniform distribution.\n\n\t\tArgs:\n\t\t\tvoltages (np.ndarray): Array of voltage values.\n\n\t\tReturns:\n\t\t\tfloat: Infinity-norm distance.\n\t\t\"\"\"\n\t\tvoltages.sort()\n\t\tuniform = np.array([x / (len(voltages) - 1) for x in range(len(voltages))])\n\t\treturn np.linalg.norm(abs(voltages - uniform))\n\n\tdef nInfExp(self, voltages: np.ndarray, base: float = 10) -&gt; float:\n\t\t\"\"\"\n\t\tComputes the infinity-norm distance between voltages and an exponential distribution.\n\n\t\tArgs:\n\t\t\tvoltages (np.ndarray): Array of voltage values.\n\t\t\tbase (float): Base of the exponential function. Defaults to 10.\n\n\t\tReturns:\n\t\t\tfloat: Infinity-norm distance.\n\t\t\"\"\"\n\t\tglobal dist\n\t\tvoltages.sort()\n\t\tif len(dist) != len(voltages):\n\t\t\tdist = np.array([np.power(base, (x / (len(voltages) - 1)) - 1) for x in range(len(voltages))])\n\t\treturn np.linalg.norm(abs(voltages - dist))\n\n\tdef median(self, voltages: np.ndarray, value: float = 0.5) -&gt; float:\n\t\t\"\"\"\n\t\tComputes the absolute difference between the median voltage and a given value.\n\n\t\tArgs:\n\t\t\tvoltages (np.ndarray): Array of voltage values.\n\t\t\tvalue (float): Value to compare the median to. Defaults to 0.5.\n\n\t\tReturns:\n\t\t\tfloat: Absolute difference from the median.\n\t\t\"\"\"\n\t\tvoltages.sort()\n\t\treturn abs(voltages[int(len(voltages) / 2)] - value)\n\n\tdef minimum(self, voltages: np.ndarray, value: float = 0.1) -&gt; float:\n\t\t\"\"\"\n\t\tComputes the absolute difference between the minimum voltage and a given value.\n\n\t\tArgs:\n\t\t\tvoltages (np.ndarray): Array of voltage values.\n\t\t\tvalue (float): Value to compare the minimum to. Defaults to 0.1.\n\n\t\tReturns:\n\t\t\tfloat: Absolute difference from the minimum.\n\t\t\"\"\"\n\t\tvoltages.sort()\n\t\treturn abs(voltages[0] - value)\n\n\tdef minWithStd(self, voltages: np.ndarray, value: float = 0.1) -&gt; float:\n\t\t\"\"\"\n\t\tComputes the normalized difference between the minimum voltage and a given value.\n\n\t\tArgs:\n\t\t\tvoltages (np.ndarray): Array of voltage values.\n\t\t\tvalue (float): Value to compare the minimum to. Defaults to 0.1.\n\n\t\tReturns:\n\t\t\tfloat: Normalized absolute difference using standard deviation.\n\t\t\"\"\"\n\t\tvoltages.sort()\n\t\treturn abs(voltages[0] - value) / np.std(voltages)\n\n\tdef expWithStd(self, voltages: np.ndarray, base: float = 10) -&gt; float:\n\t\t\"\"\"\n\t\tComputes the normalized exponential distance.\n\n\t\tArgs:\n\t\t\tvoltages (np.ndarray): Array of voltage values.\n\t\t\tbase (float): Base of the exponential. Defaults to 10.\n\n\t\tReturns:\n\t\t\tfloat: Normalized exponential distance.\n\t\t\"\"\"\n\t\treturn self.nInfExp(voltages, base) / np.std(voltages)\n\n\tdef setResistanceToGround(self, p_g: float) -&gt; None:\n\t\t\"\"\"\n\t\tSets the resistance to ground parameter.\n\n\t\tArgs:\n\t\t\tp_g (float): Resistance to ground value (logarithmic scale will be used).\n\t\t\"\"\"\n\t\tself.p_g = np.log(p_g)\n\n\tdef setKernelParameter(self, c: float) -&gt; None:\n\t\t\"\"\"\n\t\tSets the kernel parameter.\n\n\t\tArgs:\n\t\t\tc (float): Kernel parameter (logarithmic scale will be used).\n\t\t\"\"\"\n\t\tself.c = np.log(c)\n\n\tdef calculateFor(\n\t\tself,\n\t\tlandmarks: List,\n\t\tdata: Union[create_data.Data, kmeans.Partitions],\n\t\tc: float,\n\t\tp_g: float,\n\t\tapprox: bool = False,\n\t\tapprox_epsilon: Optional[float] = None,\n\t\tapprox_iters: Optional[int] = None\n\t) -&gt; Union[float, tuple[np.ndarray, voltage.Problem]]:\n\t\t\"\"\"\n\t\tCalculates voltages and applies the metric.\n\n\t\tArgs:\n\t\t\tlandmarks (List): Landmarks to add to the problem.\n\t\t\tdata (Union[create_data.Data, kmeans.Partitions]): Input data.\n\t\t\tc (float): Kernel parameter (log space).\n\t\t\tp_g (float): Resistance to ground (log space).\n\t\t\tapprox (bool): Whether to use approximation. Defaults to False.\n\t\t\tapprox_epsilon (Optional[float]): Epsilon value for approximation.\n\t\t\tapprox_iters (Optional[int]): Number of approximation iterations.\n\n\t\tReturns:\n\t\t\tUnion[float, tuple[np.ndarray, voltage.Problem]]: Metric value or voltages and problem.\n\t\t\"\"\"\n\n\t\tif isinstance(data, create_data.Data):\n\t\t\tmeanProblem = voltage.Problem(data)\n\t\t\tmeanProblem.timeStart()\n\t\t\tmeanProblem.setKernel(meanProblem.gaussiankernel)\n\t\t\tmeanProblem.setWeights(np.exp(c))\n\n\t\telif isinstance(data, kmeans.Partitions):\n\t\t\tpartitions = data\n\t\t\tmeanProblem = voltage.Problem(partitions.centers)\n\t\t\tmeanProblem.timeStart()\n\t\t\tmeanProblem.setKernel(meanProblem.gaussiankernel)\n\t\t\tmeanProblem.setPartitionWeights(partitions, np.exp(c))\n\n\t\telse:\n\t\t\traise ValueError(\"Unsupported data type\")\n\n\t\tmeanProblem.addUniversalGround(np.exp(p_g))\n\t\tmeanProblem.addLandmarks(landmarks)\n\n\t\tmeanProblem.timeEnd()\n\n\t\tif approx:\n\t\t\tvoltages = np.array(voltage.Solver(meanProblem).approximate_voltages(approx_epsilon, approx_iters))\n\t\telse:\n\t\t\tvoltages = np.array(voltage.Solver(meanProblem).compute_voltages())\n\n\t\tmeanProblem.timeEnd()\n\n\t\tif self.metric:\n\t\t\treturn self.metric(voltages)\n\t\telse:\n\t\t\treturn voltages, meanProblem\n\n\tdef bestParameterFinder(\n\t\tself,\n\t\tlandmarks: List,\n\t\tdata: Union[create_data.Data, kmeans.Partitions],\n\t\tminBound: float = -25,\n\t\tmaxBound: float = -1,\n\t\tgranularity: int = 5,\n\t\tepsilon: float = 1,\n\t\tapprox: Optional[int] = None\n\t) -&gt; tuple[float, float]:\n\t\t\"\"\"\n\t\tFinds optimal (C, P_G) parameters minimizing the metric.\n\n\t\tArgs:\n\t\t\tlandmarks (List): Landmarks to use in solving.\n\t\t\tdata (Union[create_data.Data, kmeans.Partitions]): Input dataset.\n\t\t\tminBound (float): Minimum log-bound for search. Defaults to -25.\n\t\t\tmaxBound (float): Maximum log-bound for search. Defaults to -1.\n\t\t\tgranularity (int): Granularity of grid search. Defaults to 5.\n\t\t\tepsilon (float): Precision threshold. Defaults to 1.\n\t\t\tapprox (Optional[int]): Approximation iteration count. Defaults to None.\n\n\t\tReturns:\n\t\t\ttuple[float, float]: Best (C, P_G) parameters (in real scale).\n\t\t\"\"\"\n\t\twindow_size = (maxBound - minBound) / 2\n\t\tbestc = minBound + window_size\n\t\tbestg = minBound + window_size\n\t\tval = float('inf')\n\n\t\twhile window_size &gt; epsilon:\n\t\t\tcs = [bestc + x * window_size / granularity for x in range(-granularity + 1, granularity)]\n\t\t\tgs = [bestg + x * window_size / granularity for x in range(-granularity + 1, granularity)]\n\n\t\t\tif self.c is not None:\n\t\t\t\tcs = [self.c]\n\t\t\tif self.p_g is not None:\n\t\t\t\tgs = [self.p_g]\n\n\t\t\tfor c in cs:\n\t\t\t\tfor g in gs:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tif approx is None:\n\t\t\t\t\t\t\ttempval = self.calculateFor(landmarks, data, c, g)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\ttempval = self.calculateFor(landmarks, data, c, g, approx=True, approx_iters=approx)\n\n\t\t\t\t\t\tif val &gt; tempval:\n\t\t\t\t\t\t\tbestc, bestg = c, g\n\t\t\t\t\t\t\tval = tempval\n\t\t\t\t\texcept ValueError:\n\t\t\t\t\t\tpass\n\n\t\t\twindow_size /= granularity\n\n\t\treturn np.exp(bestc), np.exp(bestg)\n\n\tdef visualizations(self, voltages: List[np.ndarray], fileStarter: str) -&gt; None:\n\t\t\"\"\"\n\t\tGenerates and saves PCA and MDS visualizations of the voltage data.\n\n\t\tArgs:\n\t\t\tvoltages (List[np.ndarray]): List of voltage arrays.\n\t\t\tfileStarter (str): File name prefix for saving plots.\n\n\t\tReturns:\n\t\t\tNone\n\t\t\"\"\"\n\n\t\tpoints = np.array(list(map(list, zip(*voltages))))\n\n\t\tpca = PCA(n_components=2)\n\t\tpoints_2d = pca.fit_transform(points)\n\n\t\tplt.scatter(points_2d[:, 0], points_2d[:, 1], s=10)\n\t\tplt.xlabel(\"PCA Component 1\")\n\t\tplt.ylabel(\"PCA Component 2\")\n\t\tplt.title(\"PCA Projection of Solver Outputs\")\n\t\tplt.savefig(fileStarter + \"_PCA.png\")\n\t\tplt.clf()\n\n\t\tmds = MDS(n_components=2, random_state=42)\n\t\ttransformed_points = mds.fit_transform(points)\n\n\t\tplt.figure(figsize=(8, 6))\n\t\tplt.scatter(transformed_points[:, 0], transformed_points[:, 1], c='blue', edgecolors='black')\n\t\tplt.xlabel(\"MDS Dimension 1\")\n\t\tplt.ylabel(\"MDS Dimension 2\")\n\t\tplt.title(\"Multidimensional Scaling (MDS) to 2D\")\n\t\tplt.savefig(fileStarter + \"_MDS.png\")\n\t\tplt.clf()\n</code></pre>"},{"location":"reference/#code.bpf.BestParameterFinder.__init__","title":"<code>__init__(metric=None)</code>","text":"<p>Initializes the BestParameterFinder.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>Optional[Callable[[BestParameterFinder, ndarray], float]]</code> <p>A custom metric function. Defaults to <code>expWithStd</code>.</p> <code>None</code> Source code in <code>code\\bpf.py</code> <pre><code>def __init__(self, metric: Optional[Callable[[\"BestParameterFinder\", np.ndarray], float]] = None):\n\t\"\"\"\n\tInitializes the BestParameterFinder.\n\n\tArgs:\n\t\tmetric (Optional[Callable[[BestParameterFinder, np.ndarray], float]]):\n\t\t\tA custom metric function. Defaults to `expWithStd`.\n\t\"\"\"\n\tself.metric = metric or self.expWithStd\n\tself.p_g: Optional[float] = None\n\tself.c: Optional[float] = None\n</code></pre>"},{"location":"reference/#code.bpf.BestParameterFinder.bestParameterFinder","title":"<code>bestParameterFinder(landmarks, data, minBound=-25, maxBound=-1, granularity=5, epsilon=1, approx=None)</code>","text":"<p>Finds optimal (C, P_G) parameters minimizing the metric.</p> <p>Parameters:</p> Name Type Description Default <code>landmarks</code> <code>List</code> <p>Landmarks to use in solving.</p> required <code>data</code> <code>Union[Data, Partitions]</code> <p>Input dataset.</p> required <code>minBound</code> <code>float</code> <p>Minimum log-bound for search. Defaults to -25.</p> <code>-25</code> <code>maxBound</code> <code>float</code> <p>Maximum log-bound for search. Defaults to -1.</p> <code>-1</code> <code>granularity</code> <code>int</code> <p>Granularity of grid search. Defaults to 5.</p> <code>5</code> <code>epsilon</code> <code>float</code> <p>Precision threshold. Defaults to 1.</p> <code>1</code> <code>approx</code> <code>Optional[int]</code> <p>Approximation iteration count. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>tuple[float, float]: Best (C, P_G) parameters (in real scale).</p> Source code in <code>code\\bpf.py</code> <pre><code>def bestParameterFinder(\n\tself,\n\tlandmarks: List,\n\tdata: Union[create_data.Data, kmeans.Partitions],\n\tminBound: float = -25,\n\tmaxBound: float = -1,\n\tgranularity: int = 5,\n\tepsilon: float = 1,\n\tapprox: Optional[int] = None\n) -&gt; tuple[float, float]:\n\t\"\"\"\n\tFinds optimal (C, P_G) parameters minimizing the metric.\n\n\tArgs:\n\t\tlandmarks (List): Landmarks to use in solving.\n\t\tdata (Union[create_data.Data, kmeans.Partitions]): Input dataset.\n\t\tminBound (float): Minimum log-bound for search. Defaults to -25.\n\t\tmaxBound (float): Maximum log-bound for search. Defaults to -1.\n\t\tgranularity (int): Granularity of grid search. Defaults to 5.\n\t\tepsilon (float): Precision threshold. Defaults to 1.\n\t\tapprox (Optional[int]): Approximation iteration count. Defaults to None.\n\n\tReturns:\n\t\ttuple[float, float]: Best (C, P_G) parameters (in real scale).\n\t\"\"\"\n\twindow_size = (maxBound - minBound) / 2\n\tbestc = minBound + window_size\n\tbestg = minBound + window_size\n\tval = float('inf')\n\n\twhile window_size &gt; epsilon:\n\t\tcs = [bestc + x * window_size / granularity for x in range(-granularity + 1, granularity)]\n\t\tgs = [bestg + x * window_size / granularity for x in range(-granularity + 1, granularity)]\n\n\t\tif self.c is not None:\n\t\t\tcs = [self.c]\n\t\tif self.p_g is not None:\n\t\t\tgs = [self.p_g]\n\n\t\tfor c in cs:\n\t\t\tfor g in gs:\n\t\t\t\ttry:\n\t\t\t\t\tif approx is None:\n\t\t\t\t\t\ttempval = self.calculateFor(landmarks, data, c, g)\n\t\t\t\t\telse:\n\t\t\t\t\t\ttempval = self.calculateFor(landmarks, data, c, g, approx=True, approx_iters=approx)\n\n\t\t\t\t\tif val &gt; tempval:\n\t\t\t\t\t\tbestc, bestg = c, g\n\t\t\t\t\t\tval = tempval\n\t\t\t\texcept ValueError:\n\t\t\t\t\tpass\n\n\t\twindow_size /= granularity\n\n\treturn np.exp(bestc), np.exp(bestg)\n</code></pre>"},{"location":"reference/#code.bpf.BestParameterFinder.calculateFor","title":"<code>calculateFor(landmarks, data, c, p_g, approx=False, approx_epsilon=None, approx_iters=None)</code>","text":"<p>Calculates voltages and applies the metric.</p> <p>Parameters:</p> Name Type Description Default <code>landmarks</code> <code>List</code> <p>Landmarks to add to the problem.</p> required <code>data</code> <code>Union[Data, Partitions]</code> <p>Input data.</p> required <code>c</code> <code>float</code> <p>Kernel parameter (log space).</p> required <code>p_g</code> <code>float</code> <p>Resistance to ground (log space).</p> required <code>approx</code> <code>bool</code> <p>Whether to use approximation. Defaults to False.</p> <code>False</code> <code>approx_epsilon</code> <code>Optional[float]</code> <p>Epsilon value for approximation.</p> <code>None</code> <code>approx_iters</code> <code>Optional[int]</code> <p>Number of approximation iterations.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[float, tuple[ndarray, Problem]]</code> <p>Union[float, tuple[np.ndarray, voltage.Problem]]: Metric value or voltages and problem.</p> Source code in <code>code\\bpf.py</code> <pre><code>def calculateFor(\n\tself,\n\tlandmarks: List,\n\tdata: Union[create_data.Data, kmeans.Partitions],\n\tc: float,\n\tp_g: float,\n\tapprox: bool = False,\n\tapprox_epsilon: Optional[float] = None,\n\tapprox_iters: Optional[int] = None\n) -&gt; Union[float, tuple[np.ndarray, voltage.Problem]]:\n\t\"\"\"\n\tCalculates voltages and applies the metric.\n\n\tArgs:\n\t\tlandmarks (List): Landmarks to add to the problem.\n\t\tdata (Union[create_data.Data, kmeans.Partitions]): Input data.\n\t\tc (float): Kernel parameter (log space).\n\t\tp_g (float): Resistance to ground (log space).\n\t\tapprox (bool): Whether to use approximation. Defaults to False.\n\t\tapprox_epsilon (Optional[float]): Epsilon value for approximation.\n\t\tapprox_iters (Optional[int]): Number of approximation iterations.\n\n\tReturns:\n\t\tUnion[float, tuple[np.ndarray, voltage.Problem]]: Metric value or voltages and problem.\n\t\"\"\"\n\n\tif isinstance(data, create_data.Data):\n\t\tmeanProblem = voltage.Problem(data)\n\t\tmeanProblem.timeStart()\n\t\tmeanProblem.setKernel(meanProblem.gaussiankernel)\n\t\tmeanProblem.setWeights(np.exp(c))\n\n\telif isinstance(data, kmeans.Partitions):\n\t\tpartitions = data\n\t\tmeanProblem = voltage.Problem(partitions.centers)\n\t\tmeanProblem.timeStart()\n\t\tmeanProblem.setKernel(meanProblem.gaussiankernel)\n\t\tmeanProblem.setPartitionWeights(partitions, np.exp(c))\n\n\telse:\n\t\traise ValueError(\"Unsupported data type\")\n\n\tmeanProblem.addUniversalGround(np.exp(p_g))\n\tmeanProblem.addLandmarks(landmarks)\n\n\tmeanProblem.timeEnd()\n\n\tif approx:\n\t\tvoltages = np.array(voltage.Solver(meanProblem).approximate_voltages(approx_epsilon, approx_iters))\n\telse:\n\t\tvoltages = np.array(voltage.Solver(meanProblem).compute_voltages())\n\n\tmeanProblem.timeEnd()\n\n\tif self.metric:\n\t\treturn self.metric(voltages)\n\telse:\n\t\treturn voltages, meanProblem\n</code></pre>"},{"location":"reference/#code.bpf.BestParameterFinder.expWithStd","title":"<code>expWithStd(voltages, base=10)</code>","text":"<p>Computes the normalized exponential distance.</p> <p>Parameters:</p> Name Type Description Default <code>voltages</code> <code>ndarray</code> <p>Array of voltage values.</p> required <code>base</code> <code>float</code> <p>Base of the exponential. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Normalized exponential distance.</p> Source code in <code>code\\bpf.py</code> <pre><code>def expWithStd(self, voltages: np.ndarray, base: float = 10) -&gt; float:\n\t\"\"\"\n\tComputes the normalized exponential distance.\n\n\tArgs:\n\t\tvoltages (np.ndarray): Array of voltage values.\n\t\tbase (float): Base of the exponential. Defaults to 10.\n\n\tReturns:\n\t\tfloat: Normalized exponential distance.\n\t\"\"\"\n\treturn self.nInfExp(voltages, base) / np.std(voltages)\n</code></pre>"},{"location":"reference/#code.bpf.BestParameterFinder.median","title":"<code>median(voltages, value=0.5)</code>","text":"<p>Computes the absolute difference between the median voltage and a given value.</p> <p>Parameters:</p> Name Type Description Default <code>voltages</code> <code>ndarray</code> <p>Array of voltage values.</p> required <code>value</code> <code>float</code> <p>Value to compare the median to. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Absolute difference from the median.</p> Source code in <code>code\\bpf.py</code> <pre><code>def median(self, voltages: np.ndarray, value: float = 0.5) -&gt; float:\n\t\"\"\"\n\tComputes the absolute difference between the median voltage and a given value.\n\n\tArgs:\n\t\tvoltages (np.ndarray): Array of voltage values.\n\t\tvalue (float): Value to compare the median to. Defaults to 0.5.\n\n\tReturns:\n\t\tfloat: Absolute difference from the median.\n\t\"\"\"\n\tvoltages.sort()\n\treturn abs(voltages[int(len(voltages) / 2)] - value)\n</code></pre>"},{"location":"reference/#code.bpf.BestParameterFinder.minWithStd","title":"<code>minWithStd(voltages, value=0.1)</code>","text":"<p>Computes the normalized difference between the minimum voltage and a given value.</p> <p>Parameters:</p> Name Type Description Default <code>voltages</code> <code>ndarray</code> <p>Array of voltage values.</p> required <code>value</code> <code>float</code> <p>Value to compare the minimum to. Defaults to 0.1.</p> <code>0.1</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Normalized absolute difference using standard deviation.</p> Source code in <code>code\\bpf.py</code> <pre><code>def minWithStd(self, voltages: np.ndarray, value: float = 0.1) -&gt; float:\n\t\"\"\"\n\tComputes the normalized difference between the minimum voltage and a given value.\n\n\tArgs:\n\t\tvoltages (np.ndarray): Array of voltage values.\n\t\tvalue (float): Value to compare the minimum to. Defaults to 0.1.\n\n\tReturns:\n\t\tfloat: Normalized absolute difference using standard deviation.\n\t\"\"\"\n\tvoltages.sort()\n\treturn abs(voltages[0] - value) / np.std(voltages)\n</code></pre>"},{"location":"reference/#code.bpf.BestParameterFinder.minimum","title":"<code>minimum(voltages, value=0.1)</code>","text":"<p>Computes the absolute difference between the minimum voltage and a given value.</p> <p>Parameters:</p> Name Type Description Default <code>voltages</code> <code>ndarray</code> <p>Array of voltage values.</p> required <code>value</code> <code>float</code> <p>Value to compare the minimum to. Defaults to 0.1.</p> <code>0.1</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Absolute difference from the minimum.</p> Source code in <code>code\\bpf.py</code> <pre><code>def minimum(self, voltages: np.ndarray, value: float = 0.1) -&gt; float:\n\t\"\"\"\n\tComputes the absolute difference between the minimum voltage and a given value.\n\n\tArgs:\n\t\tvoltages (np.ndarray): Array of voltage values.\n\t\tvalue (float): Value to compare the minimum to. Defaults to 0.1.\n\n\tReturns:\n\t\tfloat: Absolute difference from the minimum.\n\t\"\"\"\n\tvoltages.sort()\n\treturn abs(voltages[0] - value)\n</code></pre>"},{"location":"reference/#code.bpf.BestParameterFinder.nInfExp","title":"<code>nInfExp(voltages, base=10)</code>","text":"<p>Computes the infinity-norm distance between voltages and an exponential distribution.</p> <p>Parameters:</p> Name Type Description Default <code>voltages</code> <code>ndarray</code> <p>Array of voltage values.</p> required <code>base</code> <code>float</code> <p>Base of the exponential function. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Infinity-norm distance.</p> Source code in <code>code\\bpf.py</code> <pre><code>def nInfExp(self, voltages: np.ndarray, base: float = 10) -&gt; float:\n\t\"\"\"\n\tComputes the infinity-norm distance between voltages and an exponential distribution.\n\n\tArgs:\n\t\tvoltages (np.ndarray): Array of voltage values.\n\t\tbase (float): Base of the exponential function. Defaults to 10.\n\n\tReturns:\n\t\tfloat: Infinity-norm distance.\n\t\"\"\"\n\tglobal dist\n\tvoltages.sort()\n\tif len(dist) != len(voltages):\n\t\tdist = np.array([np.power(base, (x / (len(voltages) - 1)) - 1) for x in range(len(voltages))])\n\treturn np.linalg.norm(abs(voltages - dist))\n</code></pre>"},{"location":"reference/#code.bpf.BestParameterFinder.nInfUniform","title":"<code>nInfUniform(voltages)</code>","text":"<p>Computes the infinity-norm distance between voltages and a uniform distribution.</p> <p>Parameters:</p> Name Type Description Default <code>voltages</code> <code>ndarray</code> <p>Array of voltage values.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Infinity-norm distance.</p> Source code in <code>code\\bpf.py</code> <pre><code>def nInfUniform(self, voltages: np.ndarray) -&gt; float:\n\t\"\"\"\n\tComputes the infinity-norm distance between voltages and a uniform distribution.\n\n\tArgs:\n\t\tvoltages (np.ndarray): Array of voltage values.\n\n\tReturns:\n\t\tfloat: Infinity-norm distance.\n\t\"\"\"\n\tvoltages.sort()\n\tuniform = np.array([x / (len(voltages) - 1) for x in range(len(voltages))])\n\treturn np.linalg.norm(abs(voltages - uniform))\n</code></pre>"},{"location":"reference/#code.bpf.BestParameterFinder.setKernelParameter","title":"<code>setKernelParameter(c)</code>","text":"<p>Sets the kernel parameter.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>float</code> <p>Kernel parameter (logarithmic scale will be used).</p> required Source code in <code>code\\bpf.py</code> <pre><code>def setKernelParameter(self, c: float) -&gt; None:\n\t\"\"\"\n\tSets the kernel parameter.\n\n\tArgs:\n\t\tc (float): Kernel parameter (logarithmic scale will be used).\n\t\"\"\"\n\tself.c = np.log(c)\n</code></pre>"},{"location":"reference/#code.bpf.BestParameterFinder.setResistanceToGround","title":"<code>setResistanceToGround(p_g)</code>","text":"<p>Sets the resistance to ground parameter.</p> <p>Parameters:</p> Name Type Description Default <code>p_g</code> <code>float</code> <p>Resistance to ground value (logarithmic scale will be used).</p> required Source code in <code>code\\bpf.py</code> <pre><code>def setResistanceToGround(self, p_g: float) -&gt; None:\n\t\"\"\"\n\tSets the resistance to ground parameter.\n\n\tArgs:\n\t\tp_g (float): Resistance to ground value (logarithmic scale will be used).\n\t\"\"\"\n\tself.p_g = np.log(p_g)\n</code></pre>"},{"location":"reference/#code.bpf.BestParameterFinder.visualizations","title":"<code>visualizations(voltages, fileStarter)</code>","text":"<p>Generates and saves PCA and MDS visualizations of the voltage data.</p> <p>Parameters:</p> Name Type Description Default <code>voltages</code> <code>List[ndarray]</code> <p>List of voltage arrays.</p> required <code>fileStarter</code> <code>str</code> <p>File name prefix for saving plots.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>code\\bpf.py</code> <pre><code>def visualizations(self, voltages: List[np.ndarray], fileStarter: str) -&gt; None:\n\t\"\"\"\n\tGenerates and saves PCA and MDS visualizations of the voltage data.\n\n\tArgs:\n\t\tvoltages (List[np.ndarray]): List of voltage arrays.\n\t\tfileStarter (str): File name prefix for saving plots.\n\n\tReturns:\n\t\tNone\n\t\"\"\"\n\n\tpoints = np.array(list(map(list, zip(*voltages))))\n\n\tpca = PCA(n_components=2)\n\tpoints_2d = pca.fit_transform(points)\n\n\tplt.scatter(points_2d[:, 0], points_2d[:, 1], s=10)\n\tplt.xlabel(\"PCA Component 1\")\n\tplt.ylabel(\"PCA Component 2\")\n\tplt.title(\"PCA Projection of Solver Outputs\")\n\tplt.savefig(fileStarter + \"_PCA.png\")\n\tplt.clf()\n\n\tmds = MDS(n_components=2, random_state=42)\n\ttransformed_points = mds.fit_transform(points)\n\n\tplt.figure(figsize=(8, 6))\n\tplt.scatter(transformed_points[:, 0], transformed_points[:, 1], c='blue', edgecolors='black')\n\tplt.xlabel(\"MDS Dimension 1\")\n\tplt.ylabel(\"MDS Dimension 2\")\n\tplt.title(\"Multidimensional Scaling (MDS) to 2D\")\n\tplt.savefig(fileStarter + \"_MDS.png\")\n\tplt.clf()\n</code></pre>"},{"location":"reference/#code.create_data.Data","title":"<code>Data</code>","text":"<p>Class for handling and processing data sets.</p> Source code in <code>code\\create_data.py</code> <pre><code>class Data():\n\t\"\"\"Class for handling and processing data sets.\"\"\"\n\tdef __init__(self, arg=None, stream=False):\n\t\t\"\"\"\n\t\tInitializes the Data object from a list, file path, or raw data.\n\n\t\tArgs:\n\t\t\targ (Union[list, str, Any]): The input data or path to data file.\n\t\t\tstream (bool): Whether to use streaming mode for large files.\n\t\t\"\"\"\n\t\tself.stream = stream\n\n\t\tif isinstance(arg, list):\n\t\t\tself.data = np.array(arg)\n\t\t\tself.length = len(self.data)\n\t\telif isinstance(arg, str):\n\t\t\tif (stream):\n\t\t\t\tself.data = self.stream_data_json(arg)\n\t\t\t\tself.length = next(self.data)\n\t\t\t\tself.i = 0\n\t\t\telse:\n\t\t\t\tself.load_data(arg)\n\t\t\t\tself.length = len(self.data)\n\n\t\t\tself.input_file = arg\n\t\telse:\n\t\t\tself.data = arg\n\t\t\tself.length = len(self.data)\n\n\tdef __len__(self):\n\t\t\"\"\"\n\t\tReturns the length of the dataset.\n\n\t\tReturns:\n\t\t\tint: The number of data points.\n\t\t\"\"\"\n\t\treturn self.length\n\n\tdef __getitem__(self, index):\n\t\t\"\"\"\n\t\tAllows indexing into the dataset.\n\n\t\tArgs:\n\t\t\tindex (int): Index of the desired data point.\n\n\t\tReturns:\n\t\t\tnp.ndarray: The data point at the given index.\n\t\t\"\"\"\n\t\tif (self.stream):\n\t\t\tif (index &lt; self.i):\n\t\t\t\tself.data = self.stream_data_json(self.input_file)\n\t\t\t\tnext(self.data)\n\t\t\t\tself.i = 0\n\n\t\t\twhile (self.i &lt;= index):\n\t\t\t\tvalue = next(self.data)\n\t\t\t\tself.i += 1\n\n\t\t\treturn value\n\t\telse:\n\t\t\treturn self.data[index]\n\n\tdef __setitem__(self, index, value):\n\t\t\"\"\"\n\t\tSets a value in the dataset at a specified index.\n\n\t\tArgs:\n\t\t\tindex (int): The index to modify.\n\t\t\tvalue (Any): The new value to set.\n\t\t\"\"\"\n\t\tself.data[index] = value\n\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\tReturns an iterator over the dataset for use in for-loops.\n\n\t\tReturns:\n\t\t\tIterator: An iterator over the dataset.\n\t\t\"\"\"\n\t\tif (hasattr(self, 'input_file')):\n\t\t\tself.streaming_data = self.stream_data_json(self.input_file)\n\t\t\tnext(self.streaming_data)\n\t\telse:\n\t\t\tself.streaming_data = 0\n\n\t\treturn self\n\n\tdef __next__(self):\n\t\t\"\"\"\n\t\tRetrieves the next data point in an iteration.\n\n\t\tReturns:\n\t\t\tnp.ndarray: The next data point.\n\n\t\tRaises:\n\t\t\tStopIteration: If the end of the dataset is reached.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif (hasattr(self, 'input_file')):\n\t\t\t\treturn np.array(next(self.streaming_data))\n\t\t\telse:\n\t\t\t\tif (self.streaming_data == self.length):\n\t\t\t\t\traise\n\t\t\t\telse:\n\t\t\t\t\treturn np.array(self.data[self.streaming_data])\n\n\t\t\t\tself.streaming_data += 1\n\t\texcept StopIteration:\n\t\t\traise\n\n\tdef getSubSet(self, indexList):\n\t\t\"\"\"\n\t\tReturns a subset of the data given a list of indices.\n\n\t\tArgs:\n\t\t\tindexList (list[int]): List of indices to extract.\n\n\t\tReturns:\n\t\t\tData: A new Data object containing the selected subset.\n\t\t\"\"\"\n\t\tsubset = []\n\t\tfor index in indexList:\n\t\t\tsubset.append(self.data[index])\n\t\treturn Data(subset)\n\n\tdef save_data_json(self, output_file):\n\t\t\"\"\"\n\t\tSaves the dataset to a JSON file.\n\n\t\tArgs:\n\t\t\toutput_file (str): Path to the output file.\n\t\t\"\"\"\n\t\tfg = FileGenerator()\n\t\tfg.setGenerator(fg.linear_generator)\n\t\tfg.stream_save(output_file, self.data)\n\n\tdef save_data_pickle(self, output_file):\n\t\t\"\"\"\n\t\tSaves the dataset to a pickle file.\n\n\t\tArgs:\n\t\t\toutput_file (str): Path to the output file.\n\t\t\"\"\"\n\t\twith open(output_file, 'wb') as f: \n\t\t\tpickle.dump(self.data, f) \n\n\tdef load_data_json(self, input_file):\n\t\t\"\"\"\n\t\tLoads the dataset from a JSON file.\n\n\t\tArgs:\n\t\t\tinput_file (str): Path to the input file.\n\n\t\tReturns:\n\t\t\tlist[np.ndarray]: The loaded data.\n\t\t\"\"\"\n\t\twith open(input_file, 'r') as f:\n\t\t\tself.input_file = input_file\n\n\t\t\tdata = json.load(f)\n\t\t\tself.data = data[\"data\"]\n\t\t\tself.length = data[\"length\"]\n\t\t\tfor i, point in enumerate(self.data):\n\t\t\t\tself.data[i] = np.array(point)\n\n\t\t\treturn self.data\n\n\tdef load_data_pickle(self, input_file):\n\t\t\"\"\"\n\t\tLoads the dataset from a pickle file.\n\n\t\tArgs:\n\t\t\tinput_file (str): Path to the input file.\n\n\t\tReturns:\n\t\t\tAny: The loaded data.\n\t\t\"\"\"\n\t\twith open(input_file, 'r') as f:\n\t\t\tself.input_file = input_file\n\t\t\tself.data = pickle.load(f)\n\n\t\t\treturn self.data\n\n\tdef stream_data_json(self, input_file):\n\t\t\"\"\"\n\t\tStreams data from a JSON file one entry at a time.\n\n\t\tArgs:\n\t\t\tinput_file (str): Path to the input JSON file.\n\n\t\tYields:\n\t\t\tnp.ndarray: A single data point from the dataset.\n\t\t\"\"\"\n\t\twith open(input_file, 'rb') as f:\n\t\t\tf.seek(0, 2)\n\t\t\tposition = f.tell()\n\n\t\t\tvalue = \"\"\n\t\t\tread = False\n\t\t\twhile position &gt; 0:\n\t\t\t\tposition -= 1\n\t\t\t\tf.seek(position)\n\t\t\t\tbyte = f.read(1)\n\n\t\t\t\tif byte == b' ':\n\t\t\t\t\t# print(value)\n\t\t\t\t\tyield int(value)\n\t\t\t\t\tbreak\n\n\t\t\t\tif (read):\n\t\t\t\t\tvalue = byte.decode() + value\n\n\t\t\t\tif byte == b'}':\n\t\t\t\t\tread = True\n\n\t\twith open(input_file, 'r') as f:\n\t\t\tf.readline()\n\n\t\t\tfor line in f:\n\t\t\t\tif (\"length\" in line):\n\t\t\t\t\tbreak\n\n\t\t\t\tdata = json.loads(line.strip().split(']')[0] + ']')\n\t\t\t\tyield np.array(data)\n\n\tfile_function_pairs = [[\"json\", save_data_json, load_data_json], [\"pkl\", save_data_pickle, load_data_pickle]]\n\n\tdef data_function(self, file, save_or_load):\n\t\t\"\"\"\n\t\tRoutes file operation to appropriate function based on file extension.\n\n\t\tArgs:\n\t\t\tfile (str): File path.\n\t\t\tsave_or_load (int): 1 for save, 2 for load.\n\n\t\tReturns:\n\t\t\tOptional[Any]: The result of the load operation if applicable.\n\t\t\"\"\"\n\t\tif (file == None):\n\t\t\treturn\n\n\t\tfor ffp in self.file_function_pairs:\n\t\t\tif file[-len(ffp[0]):] == ffp[0]:\n\t\t\t\tif save_or_load == 1:\n\t\t\t\t\tffp[save_or_load](self.data, file)\n\t\t\t\telse:\n\t\t\t\t\treturn ffp[save_or_load](self, file)\n\n\tdef save_data(self, output_file):\n\t\t\"\"\"\n\t\tSaves the dataset to a file, choosing format by extension.\n\n\t\tArgs:\n\t\t\toutput_file (str): Path to the output file.\n\n\t\tReturns:\n\t\t\tData: Self (for chaining).\n\t\t\"\"\"\n\t\tself.data_function(output_file, 1)\n\t\treturn self\n\n\tdef load_data(self, input_file):\n\t\t\"\"\"\n\t\tLoads the dataset from a file, choosing format by extension.\n\n\t\tArgs:\n\t\t\tinput_file (str): Path to the input file.\n\n\t\tReturns:\n\t\t\tData: Self (for chaining).\n\t\t\"\"\"\n\t\tself.data_function(input_file, 2)\n\t\treturn self\n\n\tdef get_random_point(self):\n\t\t\"\"\"\n\t\tReturns a randomly selected point from the dataset.\n\n\t\tReturns:\n\t\t\tnp.ndarray: A random data point.\n\t\t\"\"\"\n\t\treturn select_random(self.data)\n\n\tdef plot(self, name=None):\n\t\t\"\"\"\n\t\tPlots the dataset using matplotlib.\n\n\t\tArgs:\n\t\t\tname (Optional[str]): File path to save the plot, if specified.\n\t\t\"\"\"\n\t\tPlotter().plotPoints(self.data, name)\n\n\tdef getNumpy(self):\n\t\t\"\"\"\n\t\tEnsures that the dataset is returned as a NumPy array.\n\n\t\tReturns:\n\t\t\tnp.ndarray: Dataset as a NumPy array.\n\t\t\"\"\"\n\t\tif isinstance(self.data, np.ndarray):\n\t\t\t# print(self.data.shape)\n\t\t\treturn self.data\n\t\telse:\n\t\t\ttemp = []\n\t\t\tfor x in self.data:\n\t\t\t\ttemp.append(np.array(x))\n\n\t\t\t# print(np.array(temp).shape)\n\t\t\treturn np.array(temp)\n</code></pre>"},{"location":"reference/#code.create_data.Data.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Allows indexing into the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the desired data point.</p> required <p>Returns:</p> Type Description <p>np.ndarray: The data point at the given index.</p> Source code in <code>code\\create_data.py</code> <pre><code>def __getitem__(self, index):\n\t\"\"\"\n\tAllows indexing into the dataset.\n\n\tArgs:\n\t\tindex (int): Index of the desired data point.\n\n\tReturns:\n\t\tnp.ndarray: The data point at the given index.\n\t\"\"\"\n\tif (self.stream):\n\t\tif (index &lt; self.i):\n\t\t\tself.data = self.stream_data_json(self.input_file)\n\t\t\tnext(self.data)\n\t\t\tself.i = 0\n\n\t\twhile (self.i &lt;= index):\n\t\t\tvalue = next(self.data)\n\t\t\tself.i += 1\n\n\t\treturn value\n\telse:\n\t\treturn self.data[index]\n</code></pre>"},{"location":"reference/#code.create_data.Data.__init__","title":"<code>__init__(arg=None, stream=False)</code>","text":"<p>Initializes the Data object from a list, file path, or raw data.</p> <p>Parameters:</p> Name Type Description Default <code>arg</code> <code>Union[list, str, Any]</code> <p>The input data or path to data file.</p> <code>None</code> <code>stream</code> <code>bool</code> <p>Whether to use streaming mode for large files.</p> <code>False</code> Source code in <code>code\\create_data.py</code> <pre><code>def __init__(self, arg=None, stream=False):\n\t\"\"\"\n\tInitializes the Data object from a list, file path, or raw data.\n\n\tArgs:\n\t\targ (Union[list, str, Any]): The input data or path to data file.\n\t\tstream (bool): Whether to use streaming mode for large files.\n\t\"\"\"\n\tself.stream = stream\n\n\tif isinstance(arg, list):\n\t\tself.data = np.array(arg)\n\t\tself.length = len(self.data)\n\telif isinstance(arg, str):\n\t\tif (stream):\n\t\t\tself.data = self.stream_data_json(arg)\n\t\t\tself.length = next(self.data)\n\t\t\tself.i = 0\n\t\telse:\n\t\t\tself.load_data(arg)\n\t\t\tself.length = len(self.data)\n\n\t\tself.input_file = arg\n\telse:\n\t\tself.data = arg\n\t\tself.length = len(self.data)\n</code></pre>"},{"location":"reference/#code.create_data.Data.__iter__","title":"<code>__iter__()</code>","text":"<p>Returns an iterator over the dataset for use in for-loops.</p> <p>Returns:</p> Name Type Description <code>Iterator</code> <p>An iterator over the dataset.</p> Source code in <code>code\\create_data.py</code> <pre><code>def __iter__(self):\n\t\"\"\"\n\tReturns an iterator over the dataset for use in for-loops.\n\n\tReturns:\n\t\tIterator: An iterator over the dataset.\n\t\"\"\"\n\tif (hasattr(self, 'input_file')):\n\t\tself.streaming_data = self.stream_data_json(self.input_file)\n\t\tnext(self.streaming_data)\n\telse:\n\t\tself.streaming_data = 0\n\n\treturn self\n</code></pre>"},{"location":"reference/#code.create_data.Data.__len__","title":"<code>__len__()</code>","text":"<p>Returns the length of the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The number of data points.</p> Source code in <code>code\\create_data.py</code> <pre><code>def __len__(self):\n\t\"\"\"\n\tReturns the length of the dataset.\n\n\tReturns:\n\t\tint: The number of data points.\n\t\"\"\"\n\treturn self.length\n</code></pre>"},{"location":"reference/#code.create_data.Data.__next__","title":"<code>__next__()</code>","text":"<p>Retrieves the next data point in an iteration.</p> <p>Returns:</p> Type Description <p>np.ndarray: The next data point.</p> <p>Raises:</p> Type Description <code>StopIteration</code> <p>If the end of the dataset is reached.</p> Source code in <code>code\\create_data.py</code> <pre><code>def __next__(self):\n\t\"\"\"\n\tRetrieves the next data point in an iteration.\n\n\tReturns:\n\t\tnp.ndarray: The next data point.\n\n\tRaises:\n\t\tStopIteration: If the end of the dataset is reached.\n\t\"\"\"\n\ttry:\n\t\tif (hasattr(self, 'input_file')):\n\t\t\treturn np.array(next(self.streaming_data))\n\t\telse:\n\t\t\tif (self.streaming_data == self.length):\n\t\t\t\traise\n\t\t\telse:\n\t\t\t\treturn np.array(self.data[self.streaming_data])\n\n\t\t\tself.streaming_data += 1\n\texcept StopIteration:\n\t\traise\n</code></pre>"},{"location":"reference/#code.create_data.Data.__setitem__","title":"<code>__setitem__(index, value)</code>","text":"<p>Sets a value in the dataset at a specified index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index to modify.</p> required <code>value</code> <code>Any</code> <p>The new value to set.</p> required Source code in <code>code\\create_data.py</code> <pre><code>def __setitem__(self, index, value):\n\t\"\"\"\n\tSets a value in the dataset at a specified index.\n\n\tArgs:\n\t\tindex (int): The index to modify.\n\t\tvalue (Any): The new value to set.\n\t\"\"\"\n\tself.data[index] = value\n</code></pre>"},{"location":"reference/#code.create_data.Data.data_function","title":"<code>data_function(file, save_or_load)</code>","text":"<p>Routes file operation to appropriate function based on file extension.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>File path.</p> required <code>save_or_load</code> <code>int</code> <p>1 for save, 2 for load.</p> required <p>Returns:</p> Type Description <p>Optional[Any]: The result of the load operation if applicable.</p> Source code in <code>code\\create_data.py</code> <pre><code>def data_function(self, file, save_or_load):\n\t\"\"\"\n\tRoutes file operation to appropriate function based on file extension.\n\n\tArgs:\n\t\tfile (str): File path.\n\t\tsave_or_load (int): 1 for save, 2 for load.\n\n\tReturns:\n\t\tOptional[Any]: The result of the load operation if applicable.\n\t\"\"\"\n\tif (file == None):\n\t\treturn\n\n\tfor ffp in self.file_function_pairs:\n\t\tif file[-len(ffp[0]):] == ffp[0]:\n\t\t\tif save_or_load == 1:\n\t\t\t\tffp[save_or_load](self.data, file)\n\t\t\telse:\n\t\t\t\treturn ffp[save_or_load](self, file)\n</code></pre>"},{"location":"reference/#code.create_data.Data.getNumpy","title":"<code>getNumpy()</code>","text":"<p>Ensures that the dataset is returned as a NumPy array.</p> <p>Returns:</p> Type Description <p>np.ndarray: Dataset as a NumPy array.</p> Source code in <code>code\\create_data.py</code> <pre><code>def getNumpy(self):\n\t\"\"\"\n\tEnsures that the dataset is returned as a NumPy array.\n\n\tReturns:\n\t\tnp.ndarray: Dataset as a NumPy array.\n\t\"\"\"\n\tif isinstance(self.data, np.ndarray):\n\t\t# print(self.data.shape)\n\t\treturn self.data\n\telse:\n\t\ttemp = []\n\t\tfor x in self.data:\n\t\t\ttemp.append(np.array(x))\n\n\t\t# print(np.array(temp).shape)\n\t\treturn np.array(temp)\n</code></pre>"},{"location":"reference/#code.create_data.Data.getSubSet","title":"<code>getSubSet(indexList)</code>","text":"<p>Returns a subset of the data given a list of indices.</p> <p>Parameters:</p> Name Type Description Default <code>indexList</code> <code>list[int]</code> <p>List of indices to extract.</p> required <p>Returns:</p> Name Type Description <code>Data</code> <p>A new Data object containing the selected subset.</p> Source code in <code>code\\create_data.py</code> <pre><code>def getSubSet(self, indexList):\n\t\"\"\"\n\tReturns a subset of the data given a list of indices.\n\n\tArgs:\n\t\tindexList (list[int]): List of indices to extract.\n\n\tReturns:\n\t\tData: A new Data object containing the selected subset.\n\t\"\"\"\n\tsubset = []\n\tfor index in indexList:\n\t\tsubset.append(self.data[index])\n\treturn Data(subset)\n</code></pre>"},{"location":"reference/#code.create_data.Data.get_random_point","title":"<code>get_random_point()</code>","text":"<p>Returns a randomly selected point from the dataset.</p> <p>Returns:</p> Type Description <p>np.ndarray: A random data point.</p> Source code in <code>code\\create_data.py</code> <pre><code>def get_random_point(self):\n\t\"\"\"\n\tReturns a randomly selected point from the dataset.\n\n\tReturns:\n\t\tnp.ndarray: A random data point.\n\t\"\"\"\n\treturn select_random(self.data)\n</code></pre>"},{"location":"reference/#code.create_data.Data.load_data","title":"<code>load_data(input_file)</code>","text":"<p>Loads the dataset from a file, choosing format by extension.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>Path to the input file.</p> required <p>Returns:</p> Name Type Description <code>Data</code> <p>Self (for chaining).</p> Source code in <code>code\\create_data.py</code> <pre><code>def load_data(self, input_file):\n\t\"\"\"\n\tLoads the dataset from a file, choosing format by extension.\n\n\tArgs:\n\t\tinput_file (str): Path to the input file.\n\n\tReturns:\n\t\tData: Self (for chaining).\n\t\"\"\"\n\tself.data_function(input_file, 2)\n\treturn self\n</code></pre>"},{"location":"reference/#code.create_data.Data.load_data_json","title":"<code>load_data_json(input_file)</code>","text":"<p>Loads the dataset from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>Path to the input file.</p> required <p>Returns:</p> Type Description <p>list[np.ndarray]: The loaded data.</p> Source code in <code>code\\create_data.py</code> <pre><code>def load_data_json(self, input_file):\n\t\"\"\"\n\tLoads the dataset from a JSON file.\n\n\tArgs:\n\t\tinput_file (str): Path to the input file.\n\n\tReturns:\n\t\tlist[np.ndarray]: The loaded data.\n\t\"\"\"\n\twith open(input_file, 'r') as f:\n\t\tself.input_file = input_file\n\n\t\tdata = json.load(f)\n\t\tself.data = data[\"data\"]\n\t\tself.length = data[\"length\"]\n\t\tfor i, point in enumerate(self.data):\n\t\t\tself.data[i] = np.array(point)\n\n\t\treturn self.data\n</code></pre>"},{"location":"reference/#code.create_data.Data.load_data_pickle","title":"<code>load_data_pickle(input_file)</code>","text":"<p>Loads the dataset from a pickle file.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>Path to the input file.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <p>The loaded data.</p> Source code in <code>code\\create_data.py</code> <pre><code>def load_data_pickle(self, input_file):\n\t\"\"\"\n\tLoads the dataset from a pickle file.\n\n\tArgs:\n\t\tinput_file (str): Path to the input file.\n\n\tReturns:\n\t\tAny: The loaded data.\n\t\"\"\"\n\twith open(input_file, 'r') as f:\n\t\tself.input_file = input_file\n\t\tself.data = pickle.load(f)\n\n\t\treturn self.data\n</code></pre>"},{"location":"reference/#code.create_data.Data.plot","title":"<code>plot(name=None)</code>","text":"<p>Plots the dataset using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>File path to save the plot, if specified.</p> <code>None</code> Source code in <code>code\\create_data.py</code> <pre><code>def plot(self, name=None):\n\t\"\"\"\n\tPlots the dataset using matplotlib.\n\n\tArgs:\n\t\tname (Optional[str]): File path to save the plot, if specified.\n\t\"\"\"\n\tPlotter().plotPoints(self.data, name)\n</code></pre>"},{"location":"reference/#code.create_data.Data.save_data","title":"<code>save_data(output_file)</code>","text":"<p>Saves the dataset to a file, choosing format by extension.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>str</code> <p>Path to the output file.</p> required <p>Returns:</p> Name Type Description <code>Data</code> <p>Self (for chaining).</p> Source code in <code>code\\create_data.py</code> <pre><code>def save_data(self, output_file):\n\t\"\"\"\n\tSaves the dataset to a file, choosing format by extension.\n\n\tArgs:\n\t\toutput_file (str): Path to the output file.\n\n\tReturns:\n\t\tData: Self (for chaining).\n\t\"\"\"\n\tself.data_function(output_file, 1)\n\treturn self\n</code></pre>"},{"location":"reference/#code.create_data.Data.save_data_json","title":"<code>save_data_json(output_file)</code>","text":"<p>Saves the dataset to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>str</code> <p>Path to the output file.</p> required Source code in <code>code\\create_data.py</code> <pre><code>def save_data_json(self, output_file):\n\t\"\"\"\n\tSaves the dataset to a JSON file.\n\n\tArgs:\n\t\toutput_file (str): Path to the output file.\n\t\"\"\"\n\tfg = FileGenerator()\n\tfg.setGenerator(fg.linear_generator)\n\tfg.stream_save(output_file, self.data)\n</code></pre>"},{"location":"reference/#code.create_data.Data.save_data_pickle","title":"<code>save_data_pickle(output_file)</code>","text":"<p>Saves the dataset to a pickle file.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>str</code> <p>Path to the output file.</p> required Source code in <code>code\\create_data.py</code> <pre><code>def save_data_pickle(self, output_file):\n\t\"\"\"\n\tSaves the dataset to a pickle file.\n\n\tArgs:\n\t\toutput_file (str): Path to the output file.\n\t\"\"\"\n\twith open(output_file, 'wb') as f: \n\t\tpickle.dump(self.data, f) \n</code></pre>"},{"location":"reference/#code.create_data.Data.stream_data_json","title":"<code>stream_data_json(input_file)</code>","text":"<p>Streams data from a JSON file one entry at a time.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>Path to the input JSON file.</p> required <p>Yields:</p> Type Description <p>np.ndarray: A single data point from the dataset.</p> Source code in <code>code\\create_data.py</code> <pre><code>def stream_data_json(self, input_file):\n\t\"\"\"\n\tStreams data from a JSON file one entry at a time.\n\n\tArgs:\n\t\tinput_file (str): Path to the input JSON file.\n\n\tYields:\n\t\tnp.ndarray: A single data point from the dataset.\n\t\"\"\"\n\twith open(input_file, 'rb') as f:\n\t\tf.seek(0, 2)\n\t\tposition = f.tell()\n\n\t\tvalue = \"\"\n\t\tread = False\n\t\twhile position &gt; 0:\n\t\t\tposition -= 1\n\t\t\tf.seek(position)\n\t\t\tbyte = f.read(1)\n\n\t\t\tif byte == b' ':\n\t\t\t\t# print(value)\n\t\t\t\tyield int(value)\n\t\t\t\tbreak\n\n\t\t\tif (read):\n\t\t\t\tvalue = byte.decode() + value\n\n\t\t\tif byte == b'}':\n\t\t\t\tread = True\n\n\twith open(input_file, 'r') as f:\n\t\tf.readline()\n\n\t\tfor line in f:\n\t\t\tif (\"length\" in line):\n\t\t\t\tbreak\n\n\t\t\tdata = json.loads(line.strip().split(']')[0] + ']')\n\t\t\tyield np.array(data)\n</code></pre>"},{"location":"reference/#code.create_data.DataCreator","title":"<code>DataCreator</code>","text":"<p>A utility class to create various synthetic datasets for testing and analysis. Interfaces with FileGenerator to optionally stream data to file.</p> <p>Attributes:</p> Name Type Description <code>fg</code> <code>FileGenerator</code> <p>An instance of FileGenerator used for generating data points.</p> Source code in <code>code\\create_data.py</code> <pre><code>class DataCreator:\n\t\"\"\"\n\tA utility class to create various synthetic datasets for testing and analysis.\n\tInterfaces with FileGenerator to optionally stream data to file.\n\n\tAttributes:\n\t\tfg (FileGenerator): An instance of FileGenerator used for generating data points.\n\t\"\"\"\n\n\tdef __init__(self):\n\t\tself.fg = FileGenerator()\n\n\tdef stream_dataset_creator(self, output_file: str, function: callable, seed: int, stream: bool, *args) -&gt; 'Data':\n\t\t\"\"\"\n\t\tCreates a dataset using the specified generator function, supporting streamed or non-streamed output.\n\n\t\tArgs:\n\t\t\toutput_file (str): File path to save the dataset.\n\t\t\tfunction (callable): Generator function to create data points.\n\t\t\tseed (int): Random seed for reproducibility.\n\t\t\tstream (bool): If True, streams data directly to the file.\n\t\t\t*args: Additional arguments passed to the generator function.\n\n\t\tReturns:\n\t\t\tData: The created dataset, either streamed or in-memory.\n\t\t\"\"\"\n\t\trandom.seed(seed)\n\n\t\tif stream:\n\t\t\tself.fg.setGenerator(function)\n\t\t\tself.fg.stream_save(output_file, *args)\n\t\t\tdata = Data(output_file, stream=True)\n\t\telse:\n\t\t\tdata = [point for point in function(*args)]\n\t\t\tdata = Data(data)\n\t\t\tdata.save_data(output_file)\n\n\t\treturn data\n\n\tdef create_dataset_line(self, output_file: str = None, start: float = 0, end: float = 1, points: int = 1000, seed: int = 42, stream: bool = False) -&gt; 'Data':\n\t\t\"\"\"\n\t\tCreates a 1D line dataset.\n\n\t\tArgs:\n\t\t\toutput_file (str): File path to save the dataset.\n\t\t\tstart (float): Starting point of the line.\n\t\t\tend (float): Ending point of the line.\n\t\t\tpoints (int): Number of data points.\n\t\t\tseed (int): Random seed.\n\t\t\tstream (bool): Whether to stream to file.\n\n\t\tReturns:\n\t\t\tData: The generated dataset.\n\t\t\"\"\"\n\t\treturn self.stream_dataset_creator(output_file, self.fg.line_generator, seed, stream, start, end, points)\n\n\tdef create_dataset_square_edge(self, output_file: str = None, p1: tuple = (0, 0), p2: tuple = (1, 1), points: int = 1000, seed: int = 42) -&gt; 'Data':\n\t\t\"\"\"\n\t\tCreates a dataset of points along the edges of a square.\n\n\t\tArgs:\n\t\t\toutput_file (str): File path to save the dataset.\n\t\t\tp1 (tuple): Bottom-left corner.\n\t\t\tp2 (tuple): Top-right corner.\n\t\t\tpoints (int): Number of data points.\n\t\t\tseed (int): Random seed.\n\n\t\tReturns:\n\t\t\tData: The generated dataset.\n\t\t\"\"\"\n\t\tdata = []\n\t\trandom.seed(seed)\n\n\t\tx_diff = p2[0] - p1[0]\n\t\ty_diff = p2[1] - p1[1]\n\n\t\tfor _ in range(points):\n\t\t\tr = random.random() * 4\n\t\t\tside = int(r)\n\t\t\tvar = r - side\n\n\t\t\tx_side = side % 2\n\t\t\ty_side = side &gt;&gt; 1\n\n\t\t\tx_rev = 1 - x_side\n\t\t\ty_rev = 1 - y_side\n\n\t\t\tvariation = np.array([var * x_side * x_diff, var * x_rev * y_diff])\n\t\t\toffset = np.array([x_rev * y_side * x_diff, x_side * y_rev * y_diff])\n\t\t\tshift = np.array(p1)\n\n\t\t\tdata.append(variation + offset + shift)\n\n\t\tdata = Data(data)\n\t\tdata.save_data(output_file)\n\t\treturn data\n\n\tdef create_dataset_square_fill(self, output_file: str = None, p1: tuple = (0, 0), p2: tuple = (1, 1), points: int = 1000, seed: int = 42) -&gt; 'Data':\n\t\t\"\"\"\n\t\tCreates a dataset of points filling a square area.\n\n\t\tArgs:\n\t\t\toutput_file (str): File path to save the dataset.\n\t\t\tp1 (tuple): Bottom-left corner.\n\t\t\tp2 (tuple): Top-right corner.\n\t\t\tpoints (int): Number of data points.\n\t\t\tseed (int): Random seed.\n\n\t\tReturns:\n\t\t\tData: The generated dataset.\n\t\t\"\"\"\n\t\tdata = []\n\t\trandom.seed(seed)\n\n\t\tx_diff = p2[0] - p1[0]\n\t\ty_diff = p2[1] - p1[1]\n\n\t\tfor _ in range(points):\n\t\t\tx_rand = random.random()\n\t\t\ty_rand = random.random()\n\t\t\tdata.append(np.array([x_diff * x_rand + p1[0], y_diff * y_rand + p1[1]]))\n\n\t\tdata = Data(data)\n\t\tdata.save_data(output_file)\n\t\treturn data\n\n\tdef create_dataset_eigth_sphere(self, output_file: str = None, radius: float = 1, x_pos: bool = True, y_pos: bool = True, z_pos: bool = True, points: int = 1000, seed: int = 42, stream: bool = False) -&gt; 'Data':\n\t\t\"\"\"\n\t\tCreates a dataset on an eighth of a sphere.\n\n\t\tArgs:\n\t\t\toutput_file (str): File path to save the dataset.\n\t\t\tradius (float): Radius of the sphere.\n\t\t\tx_pos (bool): Use positive x.\n\t\t\ty_pos (bool): Use positive y.\n\t\t\tz_pos (bool): Use positive z.\n\t\t\tpoints (int): Number of data points.\n\t\t\tseed (int): Random seed.\n\t\t\tstream (bool): Whether to stream to file.\n\n\t\tReturns:\n\t\t\tData: The generated dataset.\n\t\t\"\"\"\n\t\treturn self.stream_dataset_creator(output_file, self.fg.eigth_sphere_generator, seed, stream, radius, x_pos, y_pos, z_pos, points)\n\n\tdef create_dataset_triangle(self, output_file: str = None, edges: list = [[0, 0], [1, 1], [2, 0]], points: int = 1000, seed: int = 42, stream: bool = False) -&gt; 'Data':\n\t\t\"\"\"\n\t\tCreates a dataset of points on a triangle.\n\n\t\tArgs:\n\t\t\toutput_file (str): File path to save the dataset.\n\t\t\tedges (list): Three vertices of the triangle.\n\t\t\tpoints (int): Number of data points.\n\t\t\tseed (int): Random seed.\n\t\t\tstream (bool): Whether to stream to file.\n\n\t\tReturns:\n\t\t\tData: The generated dataset.\n\t\t\"\"\"\n\t\treturn self.stream_dataset_creator(output_file, self.fg.triangle_generator, seed, stream, edges, points)\n\n\tdef create_dataset_strong_clusters(self, output_file: str = None, internal_std: float = 1, external_std: float = 10, mean: list = [0, 0], clusters: int = 10, points: int = 1000, seed: int = 42, stream: bool = False) -&gt; 'Data':\n\t\t\"\"\"\n\t\tCreates a clustered dataset with multiple clusters.\n\n\t\tArgs:\n\t\t\toutput_file (str): File path to save the dataset.\n\t\t\tinternal_std (float): Standard deviation inside a cluster.\n\t\t\texternal_std (float): Spread of cluster centers.\n\t\t\tmean (list): Mean location for generating cluster centers.\n\t\t\tclusters (int): Number of clusters.\n\t\t\tpoints (int): Number of data points.\n\t\t\tseed (int): Random seed.\n\t\t\tstream (bool): Whether to stream to file.\n\n\t\tReturns:\n\t\t\tData: The generated dataset.\n\t\t\"\"\"\n\t\tdata = []\n\t\trandom.seed(seed)\n\t\tnp_mean = np.array(mean)\n\n\t\tcluster_centers = [varied_point(np_mean, external_std) for _ in range(clusters)]\n\n\t\tif stream:\n\t\t\tself.fg.setGenerator(self.fg.strong_cluster_generator)\n\t\t\tself.fg.stream_save(output_file, internal_std, cluster_centers, points)\n\t\t\tdata = Data(output_file, stream=True)\n\t\telse:\n\t\t\tfor p in self.fg.strong_cluster_generator(internal_std, cluster_centers, points):\n\t\t\t\tdata.append(p)\n\t\t\tdata = Data(data)\n\t\t\tdata.save_data(output_file)\n\n\t\treturn data\n\n\tdef rotate_into_dimention(self, data: 'Data', higher_dim: int = 3, seed: int = 42) -&gt; 'Data':\n\t\t\"\"\"\n\t\tRotates dataset into a higher dimensional space using random rotations.\n\n\t\tArgs:\n\t\t\tdata (Data): The dataset to rotate.\n\t\t\thigher_dim (int): Dimension to rotate into.\n\t\t\tseed (int): Random seed.\n\n\t\tReturns:\n\t\t\tData: The rotated dataset.\n\t\t\"\"\"\n\t\trotation_matrix = np.identity(higher_dim)\n\t\tif seed != -1:\n\t\t\trandom.seed(seed)\n\n\t\tfor x1 in range(higher_dim - 1):\n\t\t\tfor x2 in range(x1 + 1, higher_dim):\n\t\t\t\tangle = 2 * np.pi * random.random()\n\t\t\t\trot = np.identity(higher_dim)\n\t\t\t\trot[x1, x1] = np.cos(angle)\n\t\t\t\trot[x2, x2] = np.cos(angle)\n\t\t\t\trot[x1, x2] = np.sin(angle)\n\t\t\t\trot[x2, x1] = -np.sin(angle)\n\t\t\t\trotation_matrix = np.matmul(rotation_matrix, rot)\n\n\t\tdata.data = list(data.data)\n\t\tfor i in range(len(data)):\n\t\t\textended = np.zeros(higher_dim)\n\t\t\textended[:len(data[i])] = data[i]\n\t\t\tdata[i] = np.matmul(rotation_matrix, extended)\n\n\t\tdata.data = np.array(data.data)\n\t\treturn data\n\n\tdef create_dataset_spiral(self, output_file: str = None, radius: float = 1, center: list = [0, 0], rotations: int = 3, height: float = 10, points: int = 1000, seed: int = 42, stream: bool = False) -&gt; 'Data':\n\t\t\"\"\"\n\t\tCreates a 3D spiral dataset.\n\n\t\tArgs:\n\t\t\toutput_file (str): File path to save the dataset.\n\t\t\tradius (float): Radius of the spiral.\n\t\t\tcenter (list): Center offset.\n\t\t\trotations (int): Number of rotations.\n\t\t\theight (float): Height of the spiral.\n\t\t\tpoints (int): Number of data points.\n\t\t\tseed (int): Random seed.\n\t\t\tstream (bool): Whether to stream to file.\n\n\t\tReturns:\n\t\t\tData: The generated dataset.\n\t\t\"\"\"\n\t\treturn self.stream_dataset_creator(output_file, self.fg.spiral_generator, seed, stream, radius, center, rotations, height, points)\n</code></pre>"},{"location":"reference/#code.create_data.DataCreator.create_dataset_eigth_sphere","title":"<code>create_dataset_eigth_sphere(output_file=None, radius=1, x_pos=True, y_pos=True, z_pos=True, points=1000, seed=42, stream=False)</code>","text":"<p>Creates a dataset on an eighth of a sphere.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>str</code> <p>File path to save the dataset.</p> <code>None</code> <code>radius</code> <code>float</code> <p>Radius of the sphere.</p> <code>1</code> <code>x_pos</code> <code>bool</code> <p>Use positive x.</p> <code>True</code> <code>y_pos</code> <code>bool</code> <p>Use positive y.</p> <code>True</code> <code>z_pos</code> <code>bool</code> <p>Use positive z.</p> <code>True</code> <code>points</code> <code>int</code> <p>Number of data points.</p> <code>1000</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>42</code> <code>stream</code> <code>bool</code> <p>Whether to stream to file.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>The generated dataset.</p> Source code in <code>code\\create_data.py</code> <pre><code>def create_dataset_eigth_sphere(self, output_file: str = None, radius: float = 1, x_pos: bool = True, y_pos: bool = True, z_pos: bool = True, points: int = 1000, seed: int = 42, stream: bool = False) -&gt; 'Data':\n\t\"\"\"\n\tCreates a dataset on an eighth of a sphere.\n\n\tArgs:\n\t\toutput_file (str): File path to save the dataset.\n\t\tradius (float): Radius of the sphere.\n\t\tx_pos (bool): Use positive x.\n\t\ty_pos (bool): Use positive y.\n\t\tz_pos (bool): Use positive z.\n\t\tpoints (int): Number of data points.\n\t\tseed (int): Random seed.\n\t\tstream (bool): Whether to stream to file.\n\n\tReturns:\n\t\tData: The generated dataset.\n\t\"\"\"\n\treturn self.stream_dataset_creator(output_file, self.fg.eigth_sphere_generator, seed, stream, radius, x_pos, y_pos, z_pos, points)\n</code></pre>"},{"location":"reference/#code.create_data.DataCreator.create_dataset_line","title":"<code>create_dataset_line(output_file=None, start=0, end=1, points=1000, seed=42, stream=False)</code>","text":"<p>Creates a 1D line dataset.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>str</code> <p>File path to save the dataset.</p> <code>None</code> <code>start</code> <code>float</code> <p>Starting point of the line.</p> <code>0</code> <code>end</code> <code>float</code> <p>Ending point of the line.</p> <code>1</code> <code>points</code> <code>int</code> <p>Number of data points.</p> <code>1000</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>42</code> <code>stream</code> <code>bool</code> <p>Whether to stream to file.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>The generated dataset.</p> Source code in <code>code\\create_data.py</code> <pre><code>def create_dataset_line(self, output_file: str = None, start: float = 0, end: float = 1, points: int = 1000, seed: int = 42, stream: bool = False) -&gt; 'Data':\n\t\"\"\"\n\tCreates a 1D line dataset.\n\n\tArgs:\n\t\toutput_file (str): File path to save the dataset.\n\t\tstart (float): Starting point of the line.\n\t\tend (float): Ending point of the line.\n\t\tpoints (int): Number of data points.\n\t\tseed (int): Random seed.\n\t\tstream (bool): Whether to stream to file.\n\n\tReturns:\n\t\tData: The generated dataset.\n\t\"\"\"\n\treturn self.stream_dataset_creator(output_file, self.fg.line_generator, seed, stream, start, end, points)\n</code></pre>"},{"location":"reference/#code.create_data.DataCreator.create_dataset_spiral","title":"<code>create_dataset_spiral(output_file=None, radius=1, center=[0, 0], rotations=3, height=10, points=1000, seed=42, stream=False)</code>","text":"<p>Creates a 3D spiral dataset.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>str</code> <p>File path to save the dataset.</p> <code>None</code> <code>radius</code> <code>float</code> <p>Radius of the spiral.</p> <code>1</code> <code>center</code> <code>list</code> <p>Center offset.</p> <code>[0, 0]</code> <code>rotations</code> <code>int</code> <p>Number of rotations.</p> <code>3</code> <code>height</code> <code>float</code> <p>Height of the spiral.</p> <code>10</code> <code>points</code> <code>int</code> <p>Number of data points.</p> <code>1000</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>42</code> <code>stream</code> <code>bool</code> <p>Whether to stream to file.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>The generated dataset.</p> Source code in <code>code\\create_data.py</code> <pre><code>def create_dataset_spiral(self, output_file: str = None, radius: float = 1, center: list = [0, 0], rotations: int = 3, height: float = 10, points: int = 1000, seed: int = 42, stream: bool = False) -&gt; 'Data':\n\t\"\"\"\n\tCreates a 3D spiral dataset.\n\n\tArgs:\n\t\toutput_file (str): File path to save the dataset.\n\t\tradius (float): Radius of the spiral.\n\t\tcenter (list): Center offset.\n\t\trotations (int): Number of rotations.\n\t\theight (float): Height of the spiral.\n\t\tpoints (int): Number of data points.\n\t\tseed (int): Random seed.\n\t\tstream (bool): Whether to stream to file.\n\n\tReturns:\n\t\tData: The generated dataset.\n\t\"\"\"\n\treturn self.stream_dataset_creator(output_file, self.fg.spiral_generator, seed, stream, radius, center, rotations, height, points)\n</code></pre>"},{"location":"reference/#code.create_data.DataCreator.create_dataset_square_edge","title":"<code>create_dataset_square_edge(output_file=None, p1=(0, 0), p2=(1, 1), points=1000, seed=42)</code>","text":"<p>Creates a dataset of points along the edges of a square.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>str</code> <p>File path to save the dataset.</p> <code>None</code> <code>p1</code> <code>tuple</code> <p>Bottom-left corner.</p> <code>(0, 0)</code> <code>p2</code> <code>tuple</code> <p>Top-right corner.</p> <code>(1, 1)</code> <code>points</code> <code>int</code> <p>Number of data points.</p> <code>1000</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>42</code> <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>The generated dataset.</p> Source code in <code>code\\create_data.py</code> <pre><code>def create_dataset_square_edge(self, output_file: str = None, p1: tuple = (0, 0), p2: tuple = (1, 1), points: int = 1000, seed: int = 42) -&gt; 'Data':\n\t\"\"\"\n\tCreates a dataset of points along the edges of a square.\n\n\tArgs:\n\t\toutput_file (str): File path to save the dataset.\n\t\tp1 (tuple): Bottom-left corner.\n\t\tp2 (tuple): Top-right corner.\n\t\tpoints (int): Number of data points.\n\t\tseed (int): Random seed.\n\n\tReturns:\n\t\tData: The generated dataset.\n\t\"\"\"\n\tdata = []\n\trandom.seed(seed)\n\n\tx_diff = p2[0] - p1[0]\n\ty_diff = p2[1] - p1[1]\n\n\tfor _ in range(points):\n\t\tr = random.random() * 4\n\t\tside = int(r)\n\t\tvar = r - side\n\n\t\tx_side = side % 2\n\t\ty_side = side &gt;&gt; 1\n\n\t\tx_rev = 1 - x_side\n\t\ty_rev = 1 - y_side\n\n\t\tvariation = np.array([var * x_side * x_diff, var * x_rev * y_diff])\n\t\toffset = np.array([x_rev * y_side * x_diff, x_side * y_rev * y_diff])\n\t\tshift = np.array(p1)\n\n\t\tdata.append(variation + offset + shift)\n\n\tdata = Data(data)\n\tdata.save_data(output_file)\n\treturn data\n</code></pre>"},{"location":"reference/#code.create_data.DataCreator.create_dataset_square_fill","title":"<code>create_dataset_square_fill(output_file=None, p1=(0, 0), p2=(1, 1), points=1000, seed=42)</code>","text":"<p>Creates a dataset of points filling a square area.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>str</code> <p>File path to save the dataset.</p> <code>None</code> <code>p1</code> <code>tuple</code> <p>Bottom-left corner.</p> <code>(0, 0)</code> <code>p2</code> <code>tuple</code> <p>Top-right corner.</p> <code>(1, 1)</code> <code>points</code> <code>int</code> <p>Number of data points.</p> <code>1000</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>42</code> <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>The generated dataset.</p> Source code in <code>code\\create_data.py</code> <pre><code>def create_dataset_square_fill(self, output_file: str = None, p1: tuple = (0, 0), p2: tuple = (1, 1), points: int = 1000, seed: int = 42) -&gt; 'Data':\n\t\"\"\"\n\tCreates a dataset of points filling a square area.\n\n\tArgs:\n\t\toutput_file (str): File path to save the dataset.\n\t\tp1 (tuple): Bottom-left corner.\n\t\tp2 (tuple): Top-right corner.\n\t\tpoints (int): Number of data points.\n\t\tseed (int): Random seed.\n\n\tReturns:\n\t\tData: The generated dataset.\n\t\"\"\"\n\tdata = []\n\trandom.seed(seed)\n\n\tx_diff = p2[0] - p1[0]\n\ty_diff = p2[1] - p1[1]\n\n\tfor _ in range(points):\n\t\tx_rand = random.random()\n\t\ty_rand = random.random()\n\t\tdata.append(np.array([x_diff * x_rand + p1[0], y_diff * y_rand + p1[1]]))\n\n\tdata = Data(data)\n\tdata.save_data(output_file)\n\treturn data\n</code></pre>"},{"location":"reference/#code.create_data.DataCreator.create_dataset_strong_clusters","title":"<code>create_dataset_strong_clusters(output_file=None, internal_std=1, external_std=10, mean=[0, 0], clusters=10, points=1000, seed=42, stream=False)</code>","text":"<p>Creates a clustered dataset with multiple clusters.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>str</code> <p>File path to save the dataset.</p> <code>None</code> <code>internal_std</code> <code>float</code> <p>Standard deviation inside a cluster.</p> <code>1</code> <code>external_std</code> <code>float</code> <p>Spread of cluster centers.</p> <code>10</code> <code>mean</code> <code>list</code> <p>Mean location for generating cluster centers.</p> <code>[0, 0]</code> <code>clusters</code> <code>int</code> <p>Number of clusters.</p> <code>10</code> <code>points</code> <code>int</code> <p>Number of data points.</p> <code>1000</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>42</code> <code>stream</code> <code>bool</code> <p>Whether to stream to file.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>The generated dataset.</p> Source code in <code>code\\create_data.py</code> <pre><code>def create_dataset_strong_clusters(self, output_file: str = None, internal_std: float = 1, external_std: float = 10, mean: list = [0, 0], clusters: int = 10, points: int = 1000, seed: int = 42, stream: bool = False) -&gt; 'Data':\n\t\"\"\"\n\tCreates a clustered dataset with multiple clusters.\n\n\tArgs:\n\t\toutput_file (str): File path to save the dataset.\n\t\tinternal_std (float): Standard deviation inside a cluster.\n\t\texternal_std (float): Spread of cluster centers.\n\t\tmean (list): Mean location for generating cluster centers.\n\t\tclusters (int): Number of clusters.\n\t\tpoints (int): Number of data points.\n\t\tseed (int): Random seed.\n\t\tstream (bool): Whether to stream to file.\n\n\tReturns:\n\t\tData: The generated dataset.\n\t\"\"\"\n\tdata = []\n\trandom.seed(seed)\n\tnp_mean = np.array(mean)\n\n\tcluster_centers = [varied_point(np_mean, external_std) for _ in range(clusters)]\n\n\tif stream:\n\t\tself.fg.setGenerator(self.fg.strong_cluster_generator)\n\t\tself.fg.stream_save(output_file, internal_std, cluster_centers, points)\n\t\tdata = Data(output_file, stream=True)\n\telse:\n\t\tfor p in self.fg.strong_cluster_generator(internal_std, cluster_centers, points):\n\t\t\tdata.append(p)\n\t\tdata = Data(data)\n\t\tdata.save_data(output_file)\n\n\treturn data\n</code></pre>"},{"location":"reference/#code.create_data.DataCreator.create_dataset_triangle","title":"<code>create_dataset_triangle(output_file=None, edges=[[0, 0], [1, 1], [2, 0]], points=1000, seed=42, stream=False)</code>","text":"<p>Creates a dataset of points on a triangle.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>str</code> <p>File path to save the dataset.</p> <code>None</code> <code>edges</code> <code>list</code> <p>Three vertices of the triangle.</p> <code>[[0, 0], [1, 1], [2, 0]]</code> <code>points</code> <code>int</code> <p>Number of data points.</p> <code>1000</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>42</code> <code>stream</code> <code>bool</code> <p>Whether to stream to file.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>The generated dataset.</p> Source code in <code>code\\create_data.py</code> <pre><code>def create_dataset_triangle(self, output_file: str = None, edges: list = [[0, 0], [1, 1], [2, 0]], points: int = 1000, seed: int = 42, stream: bool = False) -&gt; 'Data':\n\t\"\"\"\n\tCreates a dataset of points on a triangle.\n\n\tArgs:\n\t\toutput_file (str): File path to save the dataset.\n\t\tedges (list): Three vertices of the triangle.\n\t\tpoints (int): Number of data points.\n\t\tseed (int): Random seed.\n\t\tstream (bool): Whether to stream to file.\n\n\tReturns:\n\t\tData: The generated dataset.\n\t\"\"\"\n\treturn self.stream_dataset_creator(output_file, self.fg.triangle_generator, seed, stream, edges, points)\n</code></pre>"},{"location":"reference/#code.create_data.DataCreator.rotate_into_dimention","title":"<code>rotate_into_dimention(data, higher_dim=3, seed=42)</code>","text":"<p>Rotates dataset into a higher dimensional space using random rotations.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>The dataset to rotate.</p> required <code>higher_dim</code> <code>int</code> <p>Dimension to rotate into.</p> <code>3</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>42</code> <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>The rotated dataset.</p> Source code in <code>code\\create_data.py</code> <pre><code>def rotate_into_dimention(self, data: 'Data', higher_dim: int = 3, seed: int = 42) -&gt; 'Data':\n\t\"\"\"\n\tRotates dataset into a higher dimensional space using random rotations.\n\n\tArgs:\n\t\tdata (Data): The dataset to rotate.\n\t\thigher_dim (int): Dimension to rotate into.\n\t\tseed (int): Random seed.\n\n\tReturns:\n\t\tData: The rotated dataset.\n\t\"\"\"\n\trotation_matrix = np.identity(higher_dim)\n\tif seed != -1:\n\t\trandom.seed(seed)\n\n\tfor x1 in range(higher_dim - 1):\n\t\tfor x2 in range(x1 + 1, higher_dim):\n\t\t\tangle = 2 * np.pi * random.random()\n\t\t\trot = np.identity(higher_dim)\n\t\t\trot[x1, x1] = np.cos(angle)\n\t\t\trot[x2, x2] = np.cos(angle)\n\t\t\trot[x1, x2] = np.sin(angle)\n\t\t\trot[x2, x1] = -np.sin(angle)\n\t\t\trotation_matrix = np.matmul(rotation_matrix, rot)\n\n\tdata.data = list(data.data)\n\tfor i in range(len(data)):\n\t\textended = np.zeros(higher_dim)\n\t\textended[:len(data[i])] = data[i]\n\t\tdata[i] = np.matmul(rotation_matrix, extended)\n\n\tdata.data = np.array(data.data)\n\treturn data\n</code></pre>"},{"location":"reference/#code.create_data.DataCreator.stream_dataset_creator","title":"<code>stream_dataset_creator(output_file, function, seed, stream, *args)</code>","text":"<p>Creates a dataset using the specified generator function, supporting streamed or non-streamed output.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>str</code> <p>File path to save the dataset.</p> required <code>function</code> <code>callable</code> <p>Generator function to create data points.</p> required <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> required <code>stream</code> <code>bool</code> <p>If True, streams data directly to the file.</p> required <code>*args</code> <p>Additional arguments passed to the generator function.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>The created dataset, either streamed or in-memory.</p> Source code in <code>code\\create_data.py</code> <pre><code>def stream_dataset_creator(self, output_file: str, function: callable, seed: int, stream: bool, *args) -&gt; 'Data':\n\t\"\"\"\n\tCreates a dataset using the specified generator function, supporting streamed or non-streamed output.\n\n\tArgs:\n\t\toutput_file (str): File path to save the dataset.\n\t\tfunction (callable): Generator function to create data points.\n\t\tseed (int): Random seed for reproducibility.\n\t\tstream (bool): If True, streams data directly to the file.\n\t\t*args: Additional arguments passed to the generator function.\n\n\tReturns:\n\t\tData: The created dataset, either streamed or in-memory.\n\t\"\"\"\n\trandom.seed(seed)\n\n\tif stream:\n\t\tself.fg.setGenerator(function)\n\t\tself.fg.stream_save(output_file, *args)\n\t\tdata = Data(output_file, stream=True)\n\telse:\n\t\tdata = [point for point in function(*args)]\n\t\tdata = Data(data)\n\t\tdata.save_data(output_file)\n\n\treturn data\n</code></pre>"},{"location":"reference/#code.create_data.FileGenerator","title":"<code>FileGenerator</code>","text":"<p>Generates files for saved data.</p> <p>This class is designed to assist in saving generated datasets in a streaming fashion. It provides several built-in generators to create synthetic datasets for use with <code>Data</code> and <code>DataCreator</code> classes.</p> Source code in <code>code\\create_data.py</code> <pre><code>class FileGenerator:\n\t\"\"\"\n\tGenerates files for saved data.\n\n\tThis class is designed to assist in saving generated datasets in a streaming\n\tfashion. It provides several built-in generators to create synthetic datasets\n\tfor use with `Data` and `DataCreator` classes.\n\t\"\"\"\n\n\tdef __init__(self):\n\t\t\"\"\"Initializes the FileGenerator.\"\"\"\n\t\tpass\n\n\tdef setGenerator(self, fn):\n\t\t\"\"\"\n\t\tSets the generator function to be used when saving data.\n\n\t\tArgs:\n\t\t\tfn (Callable): A generator function that yields data points.\n\t\t\"\"\"\n\t\tself.data_generator = fn\n\n\tdef stream_save(self, output_file: str, *args):\n\t\t\"\"\"\n\t\tSaves data to a JSON file in a streaming manner.\n\n\t\tArgs:\n\t\t\toutput_file (str): Path to the file where data will be saved.\n\t\t\t*args: Arguments to pass to the generator function.\n\n\t\tReturns:\n\t\t\tNone\n\t\t\"\"\"\n\t\twith open(output_file, \"w\") as f:\n\t\t\tf.write(\"{\\\"data\\\": [\\n\")\n\t\t\tfirst = True\n\t\t\tlength = 0\n\t\t\tfor array in self.data_generator(*args):\n\t\t\t\tif not first:\n\t\t\t\t\tf.write(\", \\n\")\n\t\t\t\tjson.dump(list(array), f)\n\t\t\t\tlength += 1\n\t\t\t\tfirst = False\n\t\t\tf.write(\"], \\n\\\"length\\\": \" + str(length) + \"}\")\n\n\tdef linear_generator(self, data: np.ndarray):\n\t\t\"\"\"\n\t\tYields data points one by one from a NumPy array.\n\n\t\tArgs:\n\t\t\tdata (np.ndarray): Input data.\n\n\t\tYields:\n\t\t\tnp.ndarray: Single data points from the array.\n\t\t\"\"\"\n\t\tfor d in data.tolist():\n\t\t\tyield d\n\n\tdef line_generator(self, start: float, end: float, points: int):\n\t\t\"\"\"\n\t\tGenerates points along a line in 1D space.\n\n\t\tArgs:\n\t\t\tstart (float): Starting point of the line.\n\t\t\tend (float): Ending point of the line.\n\t\t\tpoints (int): Number of points to generate.\n\n\t\tYields:\n\t\t\tnp.ndarray: Single-point arrays sampled along the line.\n\t\t\"\"\"\n\t\tfor _ in range(points):\n\t\t\tyield np.array([random.random() * (end - start) + start])\n\n\tdef eigth_sphere_generator(self, radius: float, x_pos: int, y_pos: int, z_pos: int, points: int):\n\t\t\"\"\"\n\t\tGenerates points on an eighth of a sphere surface.\n\n\t\tArgs:\n\t\t\tradius (float): Radius of the sphere.\n\t\t\tx_pos (int): Hemisphere direction for X (0 or 1).\n\t\t\ty_pos (int): Hemisphere direction for Y (0 or 1).\n\t\t\tz_pos (int): Hemisphere direction for Z (0 or 1).\n\t\t\tpoints (int): Number of points to generate.\n\n\t\tYields:\n\t\t\tnp.ndarray: Points on the eighth sphere surface.\n\t\t\"\"\"\n\t\tfor _ in range(points):\n\t\t\tz = random.random()\n\t\t\tangleXY = np.pi * random.random() / 2\n\t\t\tyield np.array([\n\t\t\t\tradius * np.sqrt(1 - z**2) * np.cos(angleXY) * (2 * x_pos - 1),\n\t\t\t\tradius * np.sqrt(1 - z**2) * np.sin(angleXY) * (2 * y_pos - 1),\n\t\t\t\tradius * z * (2 * z_pos - 1)\n\t\t\t])\n\n\tdef triangle_generator(self, edges: list, points: int):\n\t\t\"\"\"\n\t\tGenerates points uniformly within a triangle defined by three vertices.\n\n\t\tArgs:\n\t\t\tedges (list): A list of three points (each a list or np.ndarray) defining the triangle.\n\t\t\tpoints (int): Number of points to generate.\n\n\t\tYields:\n\t\t\tnp.ndarray: Points uniformly sampled inside the triangle.\n\t\t\"\"\"\n\t\tbase = np.array(edges[0])\n\t\tedgeDiff1 = np.array(edges[1]) - base\n\t\tedgeDiff2 = np.array(edges[2]) - base\n\t\tfor _ in range(points):\n\t\t\td1 = random.random()\n\t\t\td2 = random.random()\n\t\t\tif d1 + d2 &gt; 1:\n\t\t\t\td1 = 1 - d1\n\t\t\t\td2 = 1 - d2\n\t\t\tyield base + d1 * edgeDiff1 + d2 * edgeDiff2\n\n\tdef strong_cluster_generator(self, internal_std: float, cluster_centers: list, points: int):\n\t\t\"\"\"\n\t\tGenerates clustered points around multiple centers with specified standard deviation.\n\n\t\tArgs:\n\t\t\tinternal_std (float): Standard deviation within each cluster.\n\t\t\tcluster_centers (list): A list of cluster center points.\n\t\t\tpoints (int): Number of points to generate.\n\n\t\tYields:\n\t\t\tnp.ndarray: Points sampled from the clusters.\n\t\t\"\"\"\n\t\tc = -1\n\t\tfor p in range(points):\n\t\t\tif (p / points &gt;= c / 100):\n\t\t\t\tc += 1\n\t\t\tyield varied_point(select_random(cluster_centers), internal_std)\n\n\tdef spiral_generator(self, radius: float, center: list, rotations: int, height: float, points: int):\n\t\t\"\"\"\n\t\tGenerates points forming a 3D spiral (helix).\n\n\t\tArgs:\n\t\t\tradius (float): Radius of the spiral.\n\t\t\tcenter (list): Center offset of the spiral (not used directly in current implementation).\n\t\t\trotations (int): Number of full 360\u00b0 turns.\n\t\t\theight (float): Total height of the spiral.\n\t\t\tpoints (int): Number of points to generate.\n\n\t\tYields:\n\t\t\tnp.ndarray: Points along the spiral.\n\t\t\"\"\"\n\t\tline = 2 * np.pi * rotations\n\t\theightPerRadian = height / line\n\t\tfor _ in range(points):\n\t\t\td = random.random() * line\n\t\t\tyield np.array([\n\t\t\t\tradius * np.cos(d),\n\t\t\t\tradius * np.sin(d),\n\t\t\t\theightPerRadian * d\n\t\t\t])\n</code></pre>"},{"location":"reference/#code.create_data.FileGenerator.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the FileGenerator.</p> Source code in <code>code\\create_data.py</code> <pre><code>def __init__(self):\n\t\"\"\"Initializes the FileGenerator.\"\"\"\n\tpass\n</code></pre>"},{"location":"reference/#code.create_data.FileGenerator.eigth_sphere_generator","title":"<code>eigth_sphere_generator(radius, x_pos, y_pos, z_pos, points)</code>","text":"<p>Generates points on an eighth of a sphere surface.</p> <p>Parameters:</p> Name Type Description Default <code>radius</code> <code>float</code> <p>Radius of the sphere.</p> required <code>x_pos</code> <code>int</code> <p>Hemisphere direction for X (0 or 1).</p> required <code>y_pos</code> <code>int</code> <p>Hemisphere direction for Y (0 or 1).</p> required <code>z_pos</code> <code>int</code> <p>Hemisphere direction for Z (0 or 1).</p> required <code>points</code> <code>int</code> <p>Number of points to generate.</p> required <p>Yields:</p> Type Description <p>np.ndarray: Points on the eighth sphere surface.</p> Source code in <code>code\\create_data.py</code> <pre><code>def eigth_sphere_generator(self, radius: float, x_pos: int, y_pos: int, z_pos: int, points: int):\n\t\"\"\"\n\tGenerates points on an eighth of a sphere surface.\n\n\tArgs:\n\t\tradius (float): Radius of the sphere.\n\t\tx_pos (int): Hemisphere direction for X (0 or 1).\n\t\ty_pos (int): Hemisphere direction for Y (0 or 1).\n\t\tz_pos (int): Hemisphere direction for Z (0 or 1).\n\t\tpoints (int): Number of points to generate.\n\n\tYields:\n\t\tnp.ndarray: Points on the eighth sphere surface.\n\t\"\"\"\n\tfor _ in range(points):\n\t\tz = random.random()\n\t\tangleXY = np.pi * random.random() / 2\n\t\tyield np.array([\n\t\t\tradius * np.sqrt(1 - z**2) * np.cos(angleXY) * (2 * x_pos - 1),\n\t\t\tradius * np.sqrt(1 - z**2) * np.sin(angleXY) * (2 * y_pos - 1),\n\t\t\tradius * z * (2 * z_pos - 1)\n\t\t])\n</code></pre>"},{"location":"reference/#code.create_data.FileGenerator.line_generator","title":"<code>line_generator(start, end, points)</code>","text":"<p>Generates points along a line in 1D space.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>float</code> <p>Starting point of the line.</p> required <code>end</code> <code>float</code> <p>Ending point of the line.</p> required <code>points</code> <code>int</code> <p>Number of points to generate.</p> required <p>Yields:</p> Type Description <p>np.ndarray: Single-point arrays sampled along the line.</p> Source code in <code>code\\create_data.py</code> <pre><code>def line_generator(self, start: float, end: float, points: int):\n\t\"\"\"\n\tGenerates points along a line in 1D space.\n\n\tArgs:\n\t\tstart (float): Starting point of the line.\n\t\tend (float): Ending point of the line.\n\t\tpoints (int): Number of points to generate.\n\n\tYields:\n\t\tnp.ndarray: Single-point arrays sampled along the line.\n\t\"\"\"\n\tfor _ in range(points):\n\t\tyield np.array([random.random() * (end - start) + start])\n</code></pre>"},{"location":"reference/#code.create_data.FileGenerator.linear_generator","title":"<code>linear_generator(data)</code>","text":"<p>Yields data points one by one from a NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input data.</p> required <p>Yields:</p> Type Description <p>np.ndarray: Single data points from the array.</p> Source code in <code>code\\create_data.py</code> <pre><code>def linear_generator(self, data: np.ndarray):\n\t\"\"\"\n\tYields data points one by one from a NumPy array.\n\n\tArgs:\n\t\tdata (np.ndarray): Input data.\n\n\tYields:\n\t\tnp.ndarray: Single data points from the array.\n\t\"\"\"\n\tfor d in data.tolist():\n\t\tyield d\n</code></pre>"},{"location":"reference/#code.create_data.FileGenerator.setGenerator","title":"<code>setGenerator(fn)</code>","text":"<p>Sets the generator function to be used when saving data.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>A generator function that yields data points.</p> required Source code in <code>code\\create_data.py</code> <pre><code>def setGenerator(self, fn):\n\t\"\"\"\n\tSets the generator function to be used when saving data.\n\n\tArgs:\n\t\tfn (Callable): A generator function that yields data points.\n\t\"\"\"\n\tself.data_generator = fn\n</code></pre>"},{"location":"reference/#code.create_data.FileGenerator.spiral_generator","title":"<code>spiral_generator(radius, center, rotations, height, points)</code>","text":"<p>Generates points forming a 3D spiral (helix).</p> <p>Parameters:</p> Name Type Description Default <code>radius</code> <code>float</code> <p>Radius of the spiral.</p> required <code>center</code> <code>list</code> <p>Center offset of the spiral (not used directly in current implementation).</p> required <code>rotations</code> <code>int</code> <p>Number of full 360\u00b0 turns.</p> required <code>height</code> <code>float</code> <p>Total height of the spiral.</p> required <code>points</code> <code>int</code> <p>Number of points to generate.</p> required <p>Yields:</p> Type Description <p>np.ndarray: Points along the spiral.</p> Source code in <code>code\\create_data.py</code> <pre><code>def spiral_generator(self, radius: float, center: list, rotations: int, height: float, points: int):\n\t\"\"\"\n\tGenerates points forming a 3D spiral (helix).\n\n\tArgs:\n\t\tradius (float): Radius of the spiral.\n\t\tcenter (list): Center offset of the spiral (not used directly in current implementation).\n\t\trotations (int): Number of full 360\u00b0 turns.\n\t\theight (float): Total height of the spiral.\n\t\tpoints (int): Number of points to generate.\n\n\tYields:\n\t\tnp.ndarray: Points along the spiral.\n\t\"\"\"\n\tline = 2 * np.pi * rotations\n\theightPerRadian = height / line\n\tfor _ in range(points):\n\t\td = random.random() * line\n\t\tyield np.array([\n\t\t\tradius * np.cos(d),\n\t\t\tradius * np.sin(d),\n\t\t\theightPerRadian * d\n\t\t])\n</code></pre>"},{"location":"reference/#code.create_data.FileGenerator.stream_save","title":"<code>stream_save(output_file, *args)</code>","text":"<p>Saves data to a JSON file in a streaming manner.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>str</code> <p>Path to the file where data will be saved.</p> required <code>*args</code> <p>Arguments to pass to the generator function.</p> <code>()</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>code\\create_data.py</code> <pre><code>def stream_save(self, output_file: str, *args):\n\t\"\"\"\n\tSaves data to a JSON file in a streaming manner.\n\n\tArgs:\n\t\toutput_file (str): Path to the file where data will be saved.\n\t\t*args: Arguments to pass to the generator function.\n\n\tReturns:\n\t\tNone\n\t\"\"\"\n\twith open(output_file, \"w\") as f:\n\t\tf.write(\"{\\\"data\\\": [\\n\")\n\t\tfirst = True\n\t\tlength = 0\n\t\tfor array in self.data_generator(*args):\n\t\t\tif not first:\n\t\t\t\tf.write(\", \\n\")\n\t\t\tjson.dump(list(array), f)\n\t\t\tlength += 1\n\t\t\tfirst = False\n\t\tf.write(\"], \\n\\\"length\\\": \" + str(length) + \"}\")\n</code></pre>"},{"location":"reference/#code.create_data.FileGenerator.strong_cluster_generator","title":"<code>strong_cluster_generator(internal_std, cluster_centers, points)</code>","text":"<p>Generates clustered points around multiple centers with specified standard deviation.</p> <p>Parameters:</p> Name Type Description Default <code>internal_std</code> <code>float</code> <p>Standard deviation within each cluster.</p> required <code>cluster_centers</code> <code>list</code> <p>A list of cluster center points.</p> required <code>points</code> <code>int</code> <p>Number of points to generate.</p> required <p>Yields:</p> Type Description <p>np.ndarray: Points sampled from the clusters.</p> Source code in <code>code\\create_data.py</code> <pre><code>def strong_cluster_generator(self, internal_std: float, cluster_centers: list, points: int):\n\t\"\"\"\n\tGenerates clustered points around multiple centers with specified standard deviation.\n\n\tArgs:\n\t\tinternal_std (float): Standard deviation within each cluster.\n\t\tcluster_centers (list): A list of cluster center points.\n\t\tpoints (int): Number of points to generate.\n\n\tYields:\n\t\tnp.ndarray: Points sampled from the clusters.\n\t\"\"\"\n\tc = -1\n\tfor p in range(points):\n\t\tif (p / points &gt;= c / 100):\n\t\t\tc += 1\n\t\tyield varied_point(select_random(cluster_centers), internal_std)\n</code></pre>"},{"location":"reference/#code.create_data.FileGenerator.triangle_generator","title":"<code>triangle_generator(edges, points)</code>","text":"<p>Generates points uniformly within a triangle defined by three vertices.</p> <p>Parameters:</p> Name Type Description Default <code>edges</code> <code>list</code> <p>A list of three points (each a list or np.ndarray) defining the triangle.</p> required <code>points</code> <code>int</code> <p>Number of points to generate.</p> required <p>Yields:</p> Type Description <p>np.ndarray: Points uniformly sampled inside the triangle.</p> Source code in <code>code\\create_data.py</code> <pre><code>def triangle_generator(self, edges: list, points: int):\n\t\"\"\"\n\tGenerates points uniformly within a triangle defined by three vertices.\n\n\tArgs:\n\t\tedges (list): A list of three points (each a list or np.ndarray) defining the triangle.\n\t\tpoints (int): Number of points to generate.\n\n\tYields:\n\t\tnp.ndarray: Points uniformly sampled inside the triangle.\n\t\"\"\"\n\tbase = np.array(edges[0])\n\tedgeDiff1 = np.array(edges[1]) - base\n\tedgeDiff2 = np.array(edges[2]) - base\n\tfor _ in range(points):\n\t\td1 = random.random()\n\t\td2 = random.random()\n\t\tif d1 + d2 &gt; 1:\n\t\t\td1 = 1 - d1\n\t\t\td2 = 1 - d2\n\t\tyield base + d1 * edgeDiff1 + d2 * edgeDiff2\n</code></pre>"},{"location":"reference/#code.create_data.Plotter","title":"<code>Plotter</code>","text":"<p>Graphs the data into different formats.</p> Source code in <code>code\\create_data.py</code> <pre><code>class Plotter:\n\t\"\"\"\n\tGraphs the data into different formats.\n\t\"\"\"\n\n\tdef pointFormatting(self, points: list[np.ndarray]) -&gt; tuple[list[float], list[float], Optional[list[float]]]:\n\t\t\"\"\"\n\t\tFormats points into separate coordinate lists for plotting.\n\n\t\tArgs:\n\t\t\tpoints (list[np.ndarray]): A list of points as NumPy arrays.\n\n\t\tReturns:\n\t\t\ttuple: x, y, and optionally z coordinate lists.\n\t\t\"\"\"\n\t\tsize = len(points[0])\n\t\tx_coords = [point[0] for point in points]\n\t\tz_coords = None\n\t\tif size &gt; 1:\n\t\t\ty_coords = [point[1] for point in points]\n\t\t\tif size &gt; 2:\n\t\t\t\tz_coords = [point[2] for point in points]\n\t\telse:\n\t\t\ty_coords = [0 for point in points]\n\t\treturn (x_coords, y_coords, z_coords)\n\n\tdef plotPoints(self, points: list[np.ndarray], name: Optional[str] = None) -&gt; None:\n\t\t\"\"\"\n\t\tPlots a single set of points in 2D or 3D.\n\n\t\tArgs:\n\t\t\tpoints (list[np.ndarray]): A list of points to plot.\n\t\t\tname (Optional[str]): Optional filename to save the plot.\n\t\t\"\"\"\n\t\tself.plotPointSets([points], name)\n\n\tdef plotPointSets(self, sets: list[list[np.ndarray]], name: Optional[str] = None) -&gt; None:\n\t\t\"\"\"\n\t\tPlots multiple sets of points in different colors.\n\n\t\tArgs:\n\t\t\tsets (list[list[np.ndarray]]): A list of point sets.\n\t\t\tname (Optional[str]): Optional filename to save the plot.\n\t\t\"\"\"\n\t\tmarkers = ['o', 'v', '*']\n\t\tcolor = ['r', 'g', 'b']\n\t\tsize = len(sets[0][0])\n\t\tfig = plt.figure()\n\t\tif size == 3:\n\t\t\tax = fig.add_subplot(111, projection='3d')\n\t\telse:\n\t\t\tax = fig.add_subplot(111)\n\t\tfor i, points in enumerate(sets):\n\t\t\t(x_coords, y_coords, z_coords) = self.pointFormatting(points)\n\t\t\tif size == 3:\n\t\t\t\tax.scatter(x_coords, y_coords, z_coords, c=color[i], marker=markers[i], label='Points')\n\t\t\telse:\n\t\t\t\tax.scatter(x_coords, y_coords, c=color[i], marker=markers[i], label='Points')\n\t\tax.legend()\n\t\tif name:\n\t\t\tplt.savefig(name)\n\t\tplt.show()\n\n\tdef voltage_plot(\n\t\tself,\n\t\tsolver,\n\t\tcolor: str = 'r',\n\t\tax = None,\n\t\tshow: bool = True,\n\t\tlabel: str = \"\",\n\t\tcolored: bool = False,\n\t\tname: Optional[str] = None\n\t):\n\t\t\"\"\"\n\t\tPlots voltage data overlaid on input data using optional PCA projection.\n\n\t\tArgs:\n\t\t\tsolver: A voltage solver instance with `.problem.data` and `.voltages`.\n\t\t\tcolor (str): Color for the points if `colored` is False.\n\t\t\tax: Matplotlib axis to plot on (if provided).\n\t\t\tshow (bool): Whether to show the plot.\n\t\t\tlabel (str): Label for the legend.\n\t\t\tcolored (bool): Whether to color the points by voltage values.\n\t\t\tname (Optional[str]): Optional filename to save the plot.\n\n\t\tReturns:\n\t\t\tThe axis with the plotted data.\n\t\t\"\"\"\n\t\tdim = len(solver.problem.data[0])\n\n\t\tif ax is None:\n\t\t\tfig = plt.figure()\n\t\t\tif (dim + (not colored)) == 3:\n\t\t\t\tax = fig.add_subplot(111, projection=\"3d\")\n\t\t\telse:\n\t\t\t\tax = fig.add_subplot(111)\n\n\t\tif dim &gt; 3:\n\t\t\tpca = PCA(n_components=2)\n\t\t\tpoints_2d = pca.fit_transform(solver.problem.data)\n\t\t\tx_coords, y_coords, z_coords = points_2d[:, 0], points_2d[:, 1], None\n\t\t\tdim = 2\n\t\telse:\n\t\t\tx_coords, y_coords, z_coords = self.pointFormatting(solver.problem.data)\n\n\t\tcmap = None\n\t\tc = color\n\t\targs = [x_coords, y_coords, z_coords][:dim]\n\t\targs.append(solver.voltages)\n\n\t\tif colored:\n\t\t\tcmap = 'viridis'\n\t\t\tc = solver.voltages\n\t\t\targs = args[:-1]\n\n\t\tax.scatter(*args, c=c, cmap=cmap, marker='o', label=label)\n\n\t\tif name:\n\t\t\tplt.savefig(name)\n\t\tif show:\n\t\t\tplt.show()\n\n\t\treturn ax\n</code></pre>"},{"location":"reference/#code.create_data.Plotter.plotPointSets","title":"<code>plotPointSets(sets, name=None)</code>","text":"<p>Plots multiple sets of points in different colors.</p> <p>Parameters:</p> Name Type Description Default <code>sets</code> <code>list[list[ndarray]]</code> <p>A list of point sets.</p> required <code>name</code> <code>Optional[str]</code> <p>Optional filename to save the plot.</p> <code>None</code> Source code in <code>code\\create_data.py</code> <pre><code>def plotPointSets(self, sets: list[list[np.ndarray]], name: Optional[str] = None) -&gt; None:\n\t\"\"\"\n\tPlots multiple sets of points in different colors.\n\n\tArgs:\n\t\tsets (list[list[np.ndarray]]): A list of point sets.\n\t\tname (Optional[str]): Optional filename to save the plot.\n\t\"\"\"\n\tmarkers = ['o', 'v', '*']\n\tcolor = ['r', 'g', 'b']\n\tsize = len(sets[0][0])\n\tfig = plt.figure()\n\tif size == 3:\n\t\tax = fig.add_subplot(111, projection='3d')\n\telse:\n\t\tax = fig.add_subplot(111)\n\tfor i, points in enumerate(sets):\n\t\t(x_coords, y_coords, z_coords) = self.pointFormatting(points)\n\t\tif size == 3:\n\t\t\tax.scatter(x_coords, y_coords, z_coords, c=color[i], marker=markers[i], label='Points')\n\t\telse:\n\t\t\tax.scatter(x_coords, y_coords, c=color[i], marker=markers[i], label='Points')\n\tax.legend()\n\tif name:\n\t\tplt.savefig(name)\n\tplt.show()\n</code></pre>"},{"location":"reference/#code.create_data.Plotter.plotPoints","title":"<code>plotPoints(points, name=None)</code>","text":"<p>Plots a single set of points in 2D or 3D.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>list[ndarray]</code> <p>A list of points to plot.</p> required <code>name</code> <code>Optional[str]</code> <p>Optional filename to save the plot.</p> <code>None</code> Source code in <code>code\\create_data.py</code> <pre><code>def plotPoints(self, points: list[np.ndarray], name: Optional[str] = None) -&gt; None:\n\t\"\"\"\n\tPlots a single set of points in 2D or 3D.\n\n\tArgs:\n\t\tpoints (list[np.ndarray]): A list of points to plot.\n\t\tname (Optional[str]): Optional filename to save the plot.\n\t\"\"\"\n\tself.plotPointSets([points], name)\n</code></pre>"},{"location":"reference/#code.create_data.Plotter.pointFormatting","title":"<code>pointFormatting(points)</code>","text":"<p>Formats points into separate coordinate lists for plotting.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>list[ndarray]</code> <p>A list of points as NumPy arrays.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[list[float], list[float], Optional[list[float]]]</code> <p>x, y, and optionally z coordinate lists.</p> Source code in <code>code\\create_data.py</code> <pre><code>def pointFormatting(self, points: list[np.ndarray]) -&gt; tuple[list[float], list[float], Optional[list[float]]]:\n\t\"\"\"\n\tFormats points into separate coordinate lists for plotting.\n\n\tArgs:\n\t\tpoints (list[np.ndarray]): A list of points as NumPy arrays.\n\n\tReturns:\n\t\ttuple: x, y, and optionally z coordinate lists.\n\t\"\"\"\n\tsize = len(points[0])\n\tx_coords = [point[0] for point in points]\n\tz_coords = None\n\tif size &gt; 1:\n\t\ty_coords = [point[1] for point in points]\n\t\tif size &gt; 2:\n\t\t\tz_coords = [point[2] for point in points]\n\telse:\n\t\ty_coords = [0 for point in points]\n\treturn (x_coords, y_coords, z_coords)\n</code></pre>"},{"location":"reference/#code.create_data.Plotter.voltage_plot","title":"<code>voltage_plot(solver, color='r', ax=None, show=True, label='', colored=False, name=None)</code>","text":"<p>Plots voltage data overlaid on input data using optional PCA projection.</p> <p>Parameters:</p> Name Type Description Default <code>solver</code> <p>A voltage solver instance with <code>.problem.data</code> and <code>.voltages</code>.</p> required <code>color</code> <code>str</code> <p>Color for the points if <code>colored</code> is False.</p> <code>'r'</code> <code>ax</code> <p>Matplotlib axis to plot on (if provided).</p> <code>None</code> <code>show</code> <code>bool</code> <p>Whether to show the plot.</p> <code>True</code> <code>label</code> <code>str</code> <p>Label for the legend.</p> <code>''</code> <code>colored</code> <code>bool</code> <p>Whether to color the points by voltage values.</p> <code>False</code> <code>name</code> <code>Optional[str]</code> <p>Optional filename to save the plot.</p> <code>None</code> <p>Returns:</p> Type Description <p>The axis with the plotted data.</p> Source code in <code>code\\create_data.py</code> <pre><code>def voltage_plot(\n\tself,\n\tsolver,\n\tcolor: str = 'r',\n\tax = None,\n\tshow: bool = True,\n\tlabel: str = \"\",\n\tcolored: bool = False,\n\tname: Optional[str] = None\n):\n\t\"\"\"\n\tPlots voltage data overlaid on input data using optional PCA projection.\n\n\tArgs:\n\t\tsolver: A voltage solver instance with `.problem.data` and `.voltages`.\n\t\tcolor (str): Color for the points if `colored` is False.\n\t\tax: Matplotlib axis to plot on (if provided).\n\t\tshow (bool): Whether to show the plot.\n\t\tlabel (str): Label for the legend.\n\t\tcolored (bool): Whether to color the points by voltage values.\n\t\tname (Optional[str]): Optional filename to save the plot.\n\n\tReturns:\n\t\tThe axis with the plotted data.\n\t\"\"\"\n\tdim = len(solver.problem.data[0])\n\n\tif ax is None:\n\t\tfig = plt.figure()\n\t\tif (dim + (not colored)) == 3:\n\t\t\tax = fig.add_subplot(111, projection=\"3d\")\n\t\telse:\n\t\t\tax = fig.add_subplot(111)\n\n\tif dim &gt; 3:\n\t\tpca = PCA(n_components=2)\n\t\tpoints_2d = pca.fit_transform(solver.problem.data)\n\t\tx_coords, y_coords, z_coords = points_2d[:, 0], points_2d[:, 1], None\n\t\tdim = 2\n\telse:\n\t\tx_coords, y_coords, z_coords = self.pointFormatting(solver.problem.data)\n\n\tcmap = None\n\tc = color\n\targs = [x_coords, y_coords, z_coords][:dim]\n\targs.append(solver.voltages)\n\n\tif colored:\n\t\tcmap = 'viridis'\n\t\tc = solver.voltages\n\t\targs = args[:-1]\n\n\tax.scatter(*args, c=c, cmap=cmap, marker='o', label=label)\n\n\tif name:\n\t\tplt.savefig(name)\n\tif show:\n\t\tplt.show()\n\n\treturn ax\n</code></pre>"},{"location":"reference/#code.create_data.dimentional_variation","title":"<code>dimentional_variation(dimentions)</code>","text":"<p>Returns a NumPy array of random values from a standard normal distribution.</p> <p>Parameters:</p> Name Type Description Default <code>dimentions</code> <code>int</code> <p>Number of dimensions/values to return.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of random values sampled from the standard normal distribution.</p> Source code in <code>code\\create_data.py</code> <pre><code>def dimentional_variation(dimentions: int) -&gt; np.ndarray:\n\t\"\"\"\n\tReturns a NumPy array of random values from a standard normal distribution.\n\n\tArgs:\n\t\tdimentions (int): Number of dimensions/values to return.\n\n\tReturns:\n\t\tnp.ndarray: Array of random values sampled from the standard normal distribution.\n\t\"\"\"\n\tz_vals = []\n\tfor d in range(dimentions):\n\t\tz_vals.append(stats.norm.ppf(random.random()))\n\n\treturn np.array(z_vals)\n</code></pre>"},{"location":"reference/#code.create_data.select_random","title":"<code>select_random(array)</code>","text":"<p>Selects a random element from an array.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>list</code> <p>The array to select from.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>any</code> <p>A random element from the array.</p> Source code in <code>code\\create_data.py</code> <pre><code>def select_random(array: list) -&gt; any:\n\t\"\"\"\n\tSelects a random element from an array.\n\n\tArgs:\n\t\tarray (list): The array to select from.\n\n\tReturns:\n\t\tAny: A random element from the array.\n\t\"\"\"\n\treturn array[int(len(array) * random.random())]\n</code></pre>"},{"location":"reference/#code.create_data.varied_point","title":"<code>varied_point(mean, std)</code>","text":"<p>Returns a point that is randomly offset from the mean based on standard deviation.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>ndarray</code> <p>The mean location of the point.</p> required <code>std</code> <code>float</code> <p>Standard deviation to apply.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A randomly varied point.</p> Source code in <code>code\\create_data.py</code> <pre><code>def varied_point(mean: np.ndarray, std: float) -&gt; np.ndarray:\n\t\"\"\"\n\tReturns a point that is randomly offset from the mean based on standard deviation.\n\n\tArgs:\n\t\tmean (np.ndarray): The mean location of the point.\n\t\tstd (float): Standard deviation to apply.\n\n\tReturns:\n\t\tnp.ndarray: A randomly varied point.\n\t\"\"\"\n\treturn mean + std * dimentional_variation(len(mean))\n</code></pre>"},{"location":"reference/#code.kmeans.Partitions","title":"<code>Partitions</code>","text":"<p>               Bases: <code>DistanceBased</code></p> <p>Using K-means to partition a large dataset</p> Source code in <code>code\\kmeans.py</code> <pre><code>class Partitions(DistanceBased):\n\t\"\"\"Using K-means to partition a large dataset\"\"\"\n\tdef __init__(self, data):\n\t\tself.data = data\n\t\tsuper().__init__()\n\n\tdef k_means_plus_plus(self, k):\n\t\t\"\"\"The old k-means++ algorithm before using sci-kit\"\"\"\n\n\t\t# print(self.data.data)\n\t\tself.centers = [create_data.select_random(self.data)]\n\n\t\tfor i in range(k - 1):\n\t\t\tdistances = []\n\n\t\t\tfor point in self.data:\n\t\t\t\t# print(type(point))\n\t\t\t\t# print(type(self.centers[0]))\n\n\t\t\t\t# print(point)\n\t\t\t\t# print(self.centers[0])\n\n\t\t\t\td = self.distance(point, self.centers[0])\n\t\t\t\tfor center in self.centers:\n\t\t\t\t\td = min(d, self.distance(point, center))\n\n\t\t\t\tdistances.append(d)\n\n\t\t\tdistances = np.array(distances)\n\t\t\tdistances /= np.sum(distances)\n\n\t\t\tself.centers.append(weighted_random(self.data, distances))\n\n\t\treturn self.centers\n\n\tdef k_means(self, k, seed=42, savePointAssignments=False):\n\t\t\"\"\"Runs k-means and saves the centers and point counts. With option to save pointAssignments for voronoi drawing\"\"\"\n\t\tif (seed == -1):\n\t\t\tkmeans = KMeans(n_clusters=k, init=\"k-means++\").fit(self.data)\n\t\telse:\n\t\t\tkmeans = KMeans(n_clusters=k, random_state=int(seed), init=\"k-means++\", n_init=1).fit(self.data)\n\n\t\tself.k = k\n\t\tself.centers = kmeans.cluster_centers_\n\t\tself.point_counts = np.bincount(kmeans.labels_).tolist()\n\n\t\tif savePointAssignments:\n\t\t\tself.point_assignments = [[] for i in range(k)]\n\t\t\tfor i, point in enumerate(data):\n\t\t\t\tlabel = kmeans.labels_[i]\n\n\t\t\t\t# print(point)\n\t\t\t\t# print(self.centers[label])\n\t\t\t\t# print(self.distance(point, self.centers[label]))\n\t\t\t\tself.point_assignments[label].append([point, self.distance(point, self.centers[label])])\n\n\t\t\t# self.point_assignments = [data[kmeans.labels_ == i] for i in range(k)]\t# k times less efficient\n\t\t# self.voronoi = Voronoi(self.centers)\n\n\tdef my_k_means(self, k, seed=42, savePointAssignments=False):\n\t\t\"\"\"The old k-means algorithm\"\"\"\n\n\t\tif (seed != -1):\n\t\t\trandom.seed(seed)\n\n\t\tself.centers = self.k_means_plus_plus(k)\n\n\t\tpoint_accumulator = [np.zeros(len(self.data[0])) for i in range(k)]\n\t\tpoint_counts = [0 for i in range(k)]\n\n\t\tif (savePointAssignments):\t\t\t\t\t\t\t\t\t\t\t\t\t\t# This removes the benefit of streaming\n\t\t\tself.point_assignments = [[] for i in range(k)]\n\n\t\tfor i, point in enumerate(self.data):\n\t\t\tmin_index = 0\n\t\t\tmin_dist = self.distance(point, self.centers[0])\n\n\t\t\tfor c in range(k - 1):\n\t\t\t\tdist = self.distance(point, self.centers[c + 1])\n\t\t\t\tif (min_dist &gt; dist):\n\t\t\t\t\tmin_index = c + 1\n\t\t\t\t\tmin_dist = dist\n\n\t\t\tif (savePointAssignments):\n\t\t\t\tself.point_assignments[min_index].append([point, min_dist])\n\n\t\t\tpoint_accumulator[min_index] += point\n\t\t\tpoint_counts[min_index] += 1\n\n\t\tupdated_centers = []\n\t\tself.point_counts = []\n\n\t\tfor acc, count in zip(point_accumulator, point_counts):\n\t\t\tif (count != 0):\n\t\t\t\tupdated_centers.append(acc / count)\n\t\t\t\tself.point_counts.append(count)\n\n\t\tself.centers = updated_centers\n\t\tself.voronoi = Voronoi(self.centers)\n\n\tdef getClosestPoints(self, index):\n\t\t\"\"\"\n\t\tFinds the points whose closest points are the point indicated by the index\n\n\t\tArgs:\n\t\t\tindex (int): the index of the point\n\n\t\tReturns:\n\t\t\tList[np.ndarray]: All the points whose closest point is data[index]\n\n\t\t\"\"\"\n\t\tclosest = []\n\t\tfor i, point in enumerate(self.data):\n\t\t\tmin_index = 0\n\t\t\tmin_dist = self.distance(point, self.centers[0])\n\n\t\t\tfor c in range(len(self.centers) - 1):\n\t\t\t\tdist = self.distance(point, self.centers[c + 1])\n\t\t\t\tif (min_dist &gt; dist):\n\t\t\t\t\tmin_index = c + 1\n\t\t\t\t\tmin_dist = dist\n\n\t\t\tif (min_index == index):\n\t\t\t\tclosest.append(i)\n\n\t\treturn closest\n\n\tdef plot(self, color='r', marker='o', ax=None, name=None):\n\t\t\"\"\"Plot the kmeans\"\"\"\n\t\tplot = create_data.Plotter()\n\n\t\tsize = len(self.centers[0])\n\n\t\tif (ax == None):\n\t\t\tfig = plt.figure()\n\n\t\t\tif (size == 3):\n\t\t\t\tax = fig.add_subplot(111, projection='3d')\n\t\t\telse:\n\t\t\t\tax = fig.add_subplot(111)\n\n\t\tif (size == 3):\n\t\t\t(x_coords, y_coords, z_coords) = plot.pointFormatting(self.centers)\n\t\t\tax.scatter(x_coords, y_coords, z_coords, c=color, marker=marker, label='Centers')\n\t\telse:\n\t\t\t(x_coords, y_coords, z_coords) = plot.pointFormatting(self.data)\n\t\t\tax.scatter(x_coords, y_coords, c=color, marker=marker, label='Points')\n\n\t\t\t# voronoi_plot_2d(self.voronoi, ax=ax, show_vertices=False, line_colors='blue', line_width=1, line_alpha=0.6)\n\n\t\tax.legend()\n\n\t\tif (name):\n\t\t\tplt.savefig(name)\n\n\t\tplt.show()\n</code></pre>"},{"location":"reference/#code.kmeans.Partitions.getClosestPoints","title":"<code>getClosestPoints(index)</code>","text":"<p>Finds the points whose closest points are the point indicated by the index</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>the index of the point</p> required <p>Returns:</p> Type Description <p>List[np.ndarray]: All the points whose closest point is data[index]</p> Source code in <code>code\\kmeans.py</code> <pre><code>def getClosestPoints(self, index):\n\t\"\"\"\n\tFinds the points whose closest points are the point indicated by the index\n\n\tArgs:\n\t\tindex (int): the index of the point\n\n\tReturns:\n\t\tList[np.ndarray]: All the points whose closest point is data[index]\n\n\t\"\"\"\n\tclosest = []\n\tfor i, point in enumerate(self.data):\n\t\tmin_index = 0\n\t\tmin_dist = self.distance(point, self.centers[0])\n\n\t\tfor c in range(len(self.centers) - 1):\n\t\t\tdist = self.distance(point, self.centers[c + 1])\n\t\t\tif (min_dist &gt; dist):\n\t\t\t\tmin_index = c + 1\n\t\t\t\tmin_dist = dist\n\n\t\tif (min_index == index):\n\t\t\tclosest.append(i)\n\n\treturn closest\n</code></pre>"},{"location":"reference/#code.kmeans.Partitions.k_means","title":"<code>k_means(k, seed=42, savePointAssignments=False)</code>","text":"<p>Runs k-means and saves the centers and point counts. With option to save pointAssignments for voronoi drawing</p> Source code in <code>code\\kmeans.py</code> <pre><code>def k_means(self, k, seed=42, savePointAssignments=False):\n\t\"\"\"Runs k-means and saves the centers and point counts. With option to save pointAssignments for voronoi drawing\"\"\"\n\tif (seed == -1):\n\t\tkmeans = KMeans(n_clusters=k, init=\"k-means++\").fit(self.data)\n\telse:\n\t\tkmeans = KMeans(n_clusters=k, random_state=int(seed), init=\"k-means++\", n_init=1).fit(self.data)\n\n\tself.k = k\n\tself.centers = kmeans.cluster_centers_\n\tself.point_counts = np.bincount(kmeans.labels_).tolist()\n\n\tif savePointAssignments:\n\t\tself.point_assignments = [[] for i in range(k)]\n\t\tfor i, point in enumerate(data):\n\t\t\tlabel = kmeans.labels_[i]\n\n\t\t\t# print(point)\n\t\t\t# print(self.centers[label])\n\t\t\t# print(self.distance(point, self.centers[label]))\n\t\t\tself.point_assignments[label].append([point, self.distance(point, self.centers[label])])\n</code></pre>"},{"location":"reference/#code.kmeans.Partitions.k_means_plus_plus","title":"<code>k_means_plus_plus(k)</code>","text":"<p>The old k-means++ algorithm before using sci-kit</p> Source code in <code>code\\kmeans.py</code> <pre><code>def k_means_plus_plus(self, k):\n\t\"\"\"The old k-means++ algorithm before using sci-kit\"\"\"\n\n\t# print(self.data.data)\n\tself.centers = [create_data.select_random(self.data)]\n\n\tfor i in range(k - 1):\n\t\tdistances = []\n\n\t\tfor point in self.data:\n\t\t\t# print(type(point))\n\t\t\t# print(type(self.centers[0]))\n\n\t\t\t# print(point)\n\t\t\t# print(self.centers[0])\n\n\t\t\td = self.distance(point, self.centers[0])\n\t\t\tfor center in self.centers:\n\t\t\t\td = min(d, self.distance(point, center))\n\n\t\t\tdistances.append(d)\n\n\t\tdistances = np.array(distances)\n\t\tdistances /= np.sum(distances)\n\n\t\tself.centers.append(weighted_random(self.data, distances))\n\n\treturn self.centers\n</code></pre>"},{"location":"reference/#code.kmeans.Partitions.my_k_means","title":"<code>my_k_means(k, seed=42, savePointAssignments=False)</code>","text":"<p>The old k-means algorithm</p> Source code in <code>code\\kmeans.py</code> <pre><code>def my_k_means(self, k, seed=42, savePointAssignments=False):\n\t\"\"\"The old k-means algorithm\"\"\"\n\n\tif (seed != -1):\n\t\trandom.seed(seed)\n\n\tself.centers = self.k_means_plus_plus(k)\n\n\tpoint_accumulator = [np.zeros(len(self.data[0])) for i in range(k)]\n\tpoint_counts = [0 for i in range(k)]\n\n\tif (savePointAssignments):\t\t\t\t\t\t\t\t\t\t\t\t\t\t# This removes the benefit of streaming\n\t\tself.point_assignments = [[] for i in range(k)]\n\n\tfor i, point in enumerate(self.data):\n\t\tmin_index = 0\n\t\tmin_dist = self.distance(point, self.centers[0])\n\n\t\tfor c in range(k - 1):\n\t\t\tdist = self.distance(point, self.centers[c + 1])\n\t\t\tif (min_dist &gt; dist):\n\t\t\t\tmin_index = c + 1\n\t\t\t\tmin_dist = dist\n\n\t\tif (savePointAssignments):\n\t\t\tself.point_assignments[min_index].append([point, min_dist])\n\n\t\tpoint_accumulator[min_index] += point\n\t\tpoint_counts[min_index] += 1\n\n\tupdated_centers = []\n\tself.point_counts = []\n\n\tfor acc, count in zip(point_accumulator, point_counts):\n\t\tif (count != 0):\n\t\t\tupdated_centers.append(acc / count)\n\t\t\tself.point_counts.append(count)\n\n\tself.centers = updated_centers\n\tself.voronoi = Voronoi(self.centers)\n</code></pre>"},{"location":"reference/#code.kmeans.Partitions.plot","title":"<code>plot(color='r', marker='o', ax=None, name=None)</code>","text":"<p>Plot the kmeans</p> Source code in <code>code\\kmeans.py</code> <pre><code>def plot(self, color='r', marker='o', ax=None, name=None):\n\t\"\"\"Plot the kmeans\"\"\"\n\tplot = create_data.Plotter()\n\n\tsize = len(self.centers[0])\n\n\tif (ax == None):\n\t\tfig = plt.figure()\n\n\t\tif (size == 3):\n\t\t\tax = fig.add_subplot(111, projection='3d')\n\t\telse:\n\t\t\tax = fig.add_subplot(111)\n\n\tif (size == 3):\n\t\t(x_coords, y_coords, z_coords) = plot.pointFormatting(self.centers)\n\t\tax.scatter(x_coords, y_coords, z_coords, c=color, marker=marker, label='Centers')\n\telse:\n\t\t(x_coords, y_coords, z_coords) = plot.pointFormatting(self.data)\n\t\tax.scatter(x_coords, y_coords, c=color, marker=marker, label='Points')\n\n\t\t# voronoi_plot_2d(self.voronoi, ax=ax, show_vertices=False, line_colors='blue', line_width=1, line_alpha=0.6)\n\n\tax.legend()\n\n\tif (name):\n\t\tplt.savefig(name)\n\n\tplt.show()\n</code></pre>"},{"location":"reference/#code.voltage.Landmark","title":"<code>Landmark</code>","text":"<p>Represents a location in the dataset where a voltage will be applied.</p> <p>The <code>index</code> can refer either to an individual datapoint or a partition center.</p> Source code in <code>code\\voltage.py</code> <pre><code>class Landmark:\n\t\"\"\"\n\tRepresents a location in the dataset where a voltage will be applied.\n\n\tThe `index` can refer either to an individual datapoint or a partition center.\n\t\"\"\"\n\n\tdef __init__(self, index: int, voltage: float) -&gt; None:\n\t\t\"\"\"\n\t\tInitializes a Landmark.\n\n\t\tArgs:\n\t\t\tindex (int): Index of the datapoint or partition center.\n\t\t\tvoltage (float): Voltage to be applied at the specified index.\n\t\t\"\"\"\n\t\tself.index = index\n\t\tself.voltage = voltage\n\n\t@staticmethod\n\tdef createLandmarkClosestTo(\n\t\tdata: List[Any],\n\t\tpoint: Any,\n\t\tvoltage: float,\n\t\tdistanceFn: Optional[object] = None,\n\t\tignore: List[int] = []\n\t) -&gt; \"Landmark\":\n\t\t\"\"\"\n\t\tCreates a Landmark at the index of the datapoint in `data` closest to `point`.\n\n\t\tArgs:\n\t\t\tdata (List[Any]): The dataset to search over.\n\t\t\tpoint (Any): The reference point to find the closest datapoint to.\n\t\t\tvoltage (float): The voltage to assign to the resulting Landmark.\n\t\t\tdistanceFn (Optional[object]): A distance function with a `.distance(a, b)` method.\n\t\t\t\t\t\t\t\t\t\t   Defaults to `kmeans.DistanceBased()` if None.\n\t\t\tignore (List[int], optional): List of indices to skip during the search. Defaults to empty list.\n\n\t\tReturns:\n\t\t\tLandmark: A Landmark instance corresponding to the closest datapoint.\n\t\t\"\"\"\n\t\tif distanceFn is None:\n\t\t\tdistanceFn = kmeans.DistanceBased()\n\n\t\tmost_central_index = 0\n\t\tmindist = distanceFn.distance(data[0], point)\n\n\t\tfor index in range(1, len(data)):\n\t\t\tif index in ignore:\n\t\t\t\tcontinue\n\n\t\t\tdist = distanceFn.distance(data[index], point)\n\t\t\tif dist &lt; mindist:\n\t\t\t\tmost_central_index = index\n\t\t\t\tmindist = dist\n\n\t\treturn Landmark(most_central_index, voltage)\n</code></pre>"},{"location":"reference/#code.voltage.Landmark.__init__","title":"<code>__init__(index, voltage)</code>","text":"<p>Initializes a Landmark.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the datapoint or partition center.</p> required <code>voltage</code> <code>float</code> <p>Voltage to be applied at the specified index.</p> required Source code in <code>code\\voltage.py</code> <pre><code>def __init__(self, index: int, voltage: float) -&gt; None:\n\t\"\"\"\n\tInitializes a Landmark.\n\n\tArgs:\n\t\tindex (int): Index of the datapoint or partition center.\n\t\tvoltage (float): Voltage to be applied at the specified index.\n\t\"\"\"\n\tself.index = index\n\tself.voltage = voltage\n</code></pre>"},{"location":"reference/#code.voltage.Landmark.createLandmarkClosestTo","title":"<code>createLandmarkClosestTo(data, point, voltage, distanceFn=None, ignore=[])</code>  <code>staticmethod</code>","text":"<p>Creates a Landmark at the index of the datapoint in <code>data</code> closest to <code>point</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[Any]</code> <p>The dataset to search over.</p> required <code>point</code> <code>Any</code> <p>The reference point to find the closest datapoint to.</p> required <code>voltage</code> <code>float</code> <p>The voltage to assign to the resulting Landmark.</p> required <code>distanceFn</code> <code>Optional[object]</code> <p>A distance function with a <code>.distance(a, b)</code> method.                                                    Defaults to <code>kmeans.DistanceBased()</code> if None.</p> <code>None</code> <code>ignore</code> <code>List[int]</code> <p>List of indices to skip during the search. Defaults to empty list.</p> <code>[]</code> <p>Returns:</p> Name Type Description <code>Landmark</code> <code>Landmark</code> <p>A Landmark instance corresponding to the closest datapoint.</p> Source code in <code>code\\voltage.py</code> <pre><code>@staticmethod\ndef createLandmarkClosestTo(\n\tdata: List[Any],\n\tpoint: Any,\n\tvoltage: float,\n\tdistanceFn: Optional[object] = None,\n\tignore: List[int] = []\n) -&gt; \"Landmark\":\n\t\"\"\"\n\tCreates a Landmark at the index of the datapoint in `data` closest to `point`.\n\n\tArgs:\n\t\tdata (List[Any]): The dataset to search over.\n\t\tpoint (Any): The reference point to find the closest datapoint to.\n\t\tvoltage (float): The voltage to assign to the resulting Landmark.\n\t\tdistanceFn (Optional[object]): A distance function with a `.distance(a, b)` method.\n\t\t\t\t\t\t\t\t\t   Defaults to `kmeans.DistanceBased()` if None.\n\t\tignore (List[int], optional): List of indices to skip during the search. Defaults to empty list.\n\n\tReturns:\n\t\tLandmark: A Landmark instance corresponding to the closest datapoint.\n\t\"\"\"\n\tif distanceFn is None:\n\t\tdistanceFn = kmeans.DistanceBased()\n\n\tmost_central_index = 0\n\tmindist = distanceFn.distance(data[0], point)\n\n\tfor index in range(1, len(data)):\n\t\tif index in ignore:\n\t\t\tcontinue\n\n\t\tdist = distanceFn.distance(data[index], point)\n\t\tif dist &lt; mindist:\n\t\t\tmost_central_index = index\n\t\t\tmindist = dist\n\n\treturn Landmark(most_central_index, voltage)\n</code></pre>"},{"location":"reference/#code.voltage.Problem","title":"<code>Problem</code>","text":"<p>               Bases: <code>DistanceBased</code></p> <p>Represents the clustering/graph problem to be solved,  extending a distance-based kernel with landmarks and weights.</p> Source code in <code>code\\voltage.py</code> <pre><code>class Problem(kmeans.DistanceBased):\n\t\"\"\"\n\tRepresents the clustering/graph problem to be solved, \n\textending a distance-based kernel with landmarks and weights.\n\t\"\"\"\n\n\tdef __init__(self, data: Any) -&gt; None:\n\t\t\"\"\"\n\t\tInitializes the Problem instance.\n\n\t\tArgs:\n\t\t\tdata: An object containing your dataset. Must support len(data) \n\t\t\t\t  and data.getNumpy() to return an (n, d) numpy array.\n\t\t\"\"\"\n\t\tsuper().__init__()\n\t\tself.data = data\n\t\tself.landmarks = []\n\t\tn = len(data)\n\t\tself.weights = np.zeros([n, n])\n\t\tself.universalGround = False\n\n\tdef timeStart(self) -&gt; None:\n\t\t\"\"\"\n\t\tRecords the current time to measure elapsed intervals.\n\t\t\"\"\"\n\t\tself.start = time.time()\n\n\tdef timeEnd(self, replace: bool = True) -&gt; float:\n\t\t\"\"\"\n\t\tComputes the elapsed time since the last timeStart().\n\n\t\tArgs:\n\t\t\treplace (bool): If True, resets the start time to now.\n\n\t\tReturns:\n\t\t\tfloat: Seconds elapsed since last start.\n\t\t\"\"\"\n\t\tcur_time = time.time()\n\t\tdiff = cur_time - self.start\n\t\tif replace:\n\t\t\tself.start = cur_time\n\t\treturn diff\n\n\tdef setKernel(self, kernel: Callable[..., np.ndarray]) -&gt; None:\n\t\t\"\"\"\n\t\tSets the kernel function to use for weight computations.\n\n\t\tArgs:\n\t\t\tkernel (callable): A function or callable object with signature\n\t\t\t\t\t\t\t   kernel(X, Y, *params) \u2192 ndarray of shape (|X|, |Y|).\n\t\t\"\"\"\n\t\tself.kernel = kernel\n\n\tdef efficientSquareDistance(self, data: np.ndarray) -&gt; np.ndarray:\n\t\t\"\"\"\n\t\tComputes the pairwise squared Euclidean distances of the rows in `data`.\n\n\t\tUses the identity \u2016x\u2212y\u2016\u00b2 = \u2016x\u2016\u00b2 + \u2016y\u2016\u00b2 \u2212 2 x\u00b7y for efficiency.\n\n\t\tArgs:\n\t\t\tdata (ndarray): Array of shape (n, d).\n\n\t\tReturns:\n\t\t\tndarray: Matrix of shape (n, n) where entry (i, j) is squared distance.\n\t\t\"\"\"\n\t\tdata_norm2 = np.sum(data**2, axis=1)\n\t\tx_norm2 = data_norm2.reshape(-1, 1)\n\t\ty_norm2 = data_norm2.reshape(1, -1)\n\t\treturn x_norm2 + y_norm2 - 2 * data @ data.T\n\n\tdef radialkernel(self, data: np.ndarray, r: float) -&gt; np.ndarray:\n\t\t\"\"\"\n\t\tBuilds a binary (0/1) radial kernel: 1 if distance \u2264 r, else 0.\n\n\t\tArgs:\n\t\t\tdata (ndarray): Array of shape (n, d).\n\t\t\tr (float): Radius threshold.\n\n\t\tReturns:\n\t\t\tndarray: Adjacency-like matrix (n\u00d7n) of 0/1 floats.\n\t\t\"\"\"\n\t\tdist2 = self.efficientSquareDistance(data)\n\t\treturn (dist2 &lt;= r**2).astype(float)\n\n\tdef gaussiankernel(self, data: np.ndarray, std: float) -&gt; np.ndarray:\n\t\t\"\"\"\n\t\tBuilds a Gaussian (RBF) kernel matrix.\n\n\t\tArgs:\n\t\t\tdata (ndarray): Array of shape (n, d).\n\t\t\tstd (float): Standard deviation parameter for the Gaussian.\n\n\t\tReturns:\n\t\t\tndarray: Kernel matrix of shape (n, n).\n\t\t\"\"\"\n\t\tdist2 = self.efficientSquareDistance(data)\n\t\treturn np.exp(-dist2 / (2 * std**2))\n\n\tdef setWeights(self, *c: Any) -&gt; np.ndarray:\n\t\t\"\"\"\n\t\tComputes and normalizes the weight matrix on the original data.\n\n\t\tArgs:\n\t\t\t*c: Parameters to pass into the currently set kernel function.\n\n\t\tReturns:\n\t\t\tndarray: The normalized weight matrix (n\u00d7n).\n\t\t\"\"\"\n\t\tdata_np = self.data.getNumpy()\n\t\tn = len(self.data)\n\t\tself.weights[:n, :n] = self.kernel(data_np, *c)\n\t\tself.normalizeWeights()\n\t\treturn self.weights\n\n\tdef normalizeWeights(self) -&gt; None:\n\t\t\"\"\"\n\t\tNormalizes each row of the weight matrix to sum to 1.\n\n\t\tRaises:\n\t\t\tValueError: If any row sums to zero, resulting in NaNs.\n\t\t\"\"\"\n\t\tself.weights = self.weights / self.weights.sum(axis=1, keepdims=True)\n\t\tif np.isnan(self.weights).any():\n\t\t\traise ValueError(\"Array contains NaN values!\")\n\n\tdef setPartitionWeights(self, partition: Any, *c: Any) -&gt; np.ndarray:\n\t\t\"\"\"\n\t\tComputes and normalizes weights based on cluster centers and sizes.\n\n\t\tArgs:\n\t\t\tpartition: An object with attributes `centers` (list of points)\n\t\t\t\t\t   and `point_counts` (counts per center).\n\t\t\t*c: Parameters to pass into the kernel function.\n\n\t\tReturns:\n\t\t\tndarray: The normalized weight matrix for the partition block.\n\t\t\"\"\"\n\t\tcenters = np.array(partition.centers)\n\t\tcounts = np.array(partition.point_counts).reshape(-1, 1)\n\t\tK = self.kernel(centers[:, None], centers[None, :], *c)\n\t\tW = K * (counts @ counts.T)\n\t\tn = len(centers)\n\t\tself.weights[:n, :n] = W\n\t\tself.normalizeWeights()\n\t\treturn self.weights\n\n\tdef addUniversalGround(self, p_g: float = 0.01) -&gt; np.ndarray:\n\t\t\"\"\"\n\t\tAdds (or updates) a 'universal ground' node connected uniformly to all others.\n\n\t\tArgs:\n\t\t\tp_g (float): Total ground connection probability to distribute.\n\n\t\tReturns:\n\t\t\tndarray: The updated normalized weight matrix including the ground node.\n\t\t\"\"\"\n\t\tif self.universalGround:\n\t\t\tn = self.weights.shape[0] - 1\n\t\t\tfor x in range(n):\n\t\t\t\tself.weights[x, n] = p_g / n\n\t\t\t\tself.weights[n, x] = p_g / n\n\t\telse:\n\t\t\tself.universalGround = True\n\t\t\tn = self.weights.shape[0]\n\t\t\tnewW = np.zeros([n + 1, n + 1])\n\t\t\tnewW[:n, :n] = self.weights\n\t\t\tfor x in range(n):\n\t\t\t\tnewW[x, n] = p_g / n\n\t\t\t\tnewW[n, x] = p_g / n\n\t\t\tself.weights = newW\n\t\t\tself.addLandmark(Landmark(n, 0))\n\t\tself.normalizeWeights()\n\t\treturn self.weights\n\n\tdef addLandmark(self, landmark: Landmark) -&gt; None:\n\t\t\"\"\"\n\t\tAdds a single Landmark to the problem.\n\n\t\tArgs:\n\t\t\tlandmark (Landmark): The landmark instance to append.\n\t\t\"\"\"\n\t\tself.landmarks.append(landmark)\n\n\tdef addLandmarks(self, landmarks: List[Landmark]) -&gt; None:\n\t\t\"\"\"\n\t\tAdds multiple Landmark instances to the problem.\n\n\t\tArgs:\n\t\t\tlandmarks (List[Landmark]): List of landmarks to append.\n\t\t\"\"\"\n\t\tself.landmarks += landmarks\n\n\tdef addLandmarksInRange(\n\t\tself, minRange: Union[List[float], np.ndarray],\n\t\tmaxRange: Union[List[float], np.ndarray],\n\t\tvoltage: float\n\t) -&gt; List[Landmark]:\n\t\t\"\"\"\n\t\tAdds landmarks for all data points within a given coordinate range.\n\n\t\tArgs:\n\t\t\tminRange (array-like): Minimum bounds per dimension.\n\t\t\tmaxRange (array-like): Maximum bounds per dimension.\n\t\t\tvoltage (float): Voltage to apply at each new landmark.\n\n\t\tReturns:\n\t\t\tList[Landmark]: The list of newly added landmarks.\n\t\t\"\"\"\n\t\tadding = []\n\t\tdata_np = self.data.getNumpy()\n\t\tfor idx, point in enumerate(data_np):\n\t\t\tif np.all(point &gt;= minRange) and np.all(point &lt;= maxRange):\n\t\t\t\tadding.append(Landmark(idx, voltage))\n\t\tself.addLandmarks(adding)\n\t\treturn adding\n</code></pre>"},{"location":"reference/#code.voltage.Problem.__init__","title":"<code>__init__(data)</code>","text":"<p>Initializes the Problem instance.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>An object containing your dataset. Must support len(data)    and data.getNumpy() to return an (n, d) numpy array.</p> required Source code in <code>code\\voltage.py</code> <pre><code>def __init__(self, data: Any) -&gt; None:\n\t\"\"\"\n\tInitializes the Problem instance.\n\n\tArgs:\n\t\tdata: An object containing your dataset. Must support len(data) \n\t\t\t  and data.getNumpy() to return an (n, d) numpy array.\n\t\"\"\"\n\tsuper().__init__()\n\tself.data = data\n\tself.landmarks = []\n\tn = len(data)\n\tself.weights = np.zeros([n, n])\n\tself.universalGround = False\n</code></pre>"},{"location":"reference/#code.voltage.Problem.addLandmark","title":"<code>addLandmark(landmark)</code>","text":"<p>Adds a single Landmark to the problem.</p> <p>Parameters:</p> Name Type Description Default <code>landmark</code> <code>Landmark</code> <p>The landmark instance to append.</p> required Source code in <code>code\\voltage.py</code> <pre><code>def addLandmark(self, landmark: Landmark) -&gt; None:\n\t\"\"\"\n\tAdds a single Landmark to the problem.\n\n\tArgs:\n\t\tlandmark (Landmark): The landmark instance to append.\n\t\"\"\"\n\tself.landmarks.append(landmark)\n</code></pre>"},{"location":"reference/#code.voltage.Problem.addLandmarks","title":"<code>addLandmarks(landmarks)</code>","text":"<p>Adds multiple Landmark instances to the problem.</p> <p>Parameters:</p> Name Type Description Default <code>landmarks</code> <code>List[Landmark]</code> <p>List of landmarks to append.</p> required Source code in <code>code\\voltage.py</code> <pre><code>def addLandmarks(self, landmarks: List[Landmark]) -&gt; None:\n\t\"\"\"\n\tAdds multiple Landmark instances to the problem.\n\n\tArgs:\n\t\tlandmarks (List[Landmark]): List of landmarks to append.\n\t\"\"\"\n\tself.landmarks += landmarks\n</code></pre>"},{"location":"reference/#code.voltage.Problem.addLandmarksInRange","title":"<code>addLandmarksInRange(minRange, maxRange, voltage)</code>","text":"<p>Adds landmarks for all data points within a given coordinate range.</p> <p>Parameters:</p> Name Type Description Default <code>minRange</code> <code>array - like</code> <p>Minimum bounds per dimension.</p> required <code>maxRange</code> <code>array - like</code> <p>Maximum bounds per dimension.</p> required <code>voltage</code> <code>float</code> <p>Voltage to apply at each new landmark.</p> required <p>Returns:</p> Type Description <code>List[Landmark]</code> <p>List[Landmark]: The list of newly added landmarks.</p> Source code in <code>code\\voltage.py</code> <pre><code>def addLandmarksInRange(\n\tself, minRange: Union[List[float], np.ndarray],\n\tmaxRange: Union[List[float], np.ndarray],\n\tvoltage: float\n) -&gt; List[Landmark]:\n\t\"\"\"\n\tAdds landmarks for all data points within a given coordinate range.\n\n\tArgs:\n\t\tminRange (array-like): Minimum bounds per dimension.\n\t\tmaxRange (array-like): Maximum bounds per dimension.\n\t\tvoltage (float): Voltage to apply at each new landmark.\n\n\tReturns:\n\t\tList[Landmark]: The list of newly added landmarks.\n\t\"\"\"\n\tadding = []\n\tdata_np = self.data.getNumpy()\n\tfor idx, point in enumerate(data_np):\n\t\tif np.all(point &gt;= minRange) and np.all(point &lt;= maxRange):\n\t\t\tadding.append(Landmark(idx, voltage))\n\tself.addLandmarks(adding)\n\treturn adding\n</code></pre>"},{"location":"reference/#code.voltage.Problem.addUniversalGround","title":"<code>addUniversalGround(p_g=0.01)</code>","text":"<p>Adds (or updates) a 'universal ground' node connected uniformly to all others.</p> <p>Parameters:</p> Name Type Description Default <code>p_g</code> <code>float</code> <p>Total ground connection probability to distribute.</p> <code>0.01</code> <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The updated normalized weight matrix including the ground node.</p> Source code in <code>code\\voltage.py</code> <pre><code>def addUniversalGround(self, p_g: float = 0.01) -&gt; np.ndarray:\n\t\"\"\"\n\tAdds (or updates) a 'universal ground' node connected uniformly to all others.\n\n\tArgs:\n\t\tp_g (float): Total ground connection probability to distribute.\n\n\tReturns:\n\t\tndarray: The updated normalized weight matrix including the ground node.\n\t\"\"\"\n\tif self.universalGround:\n\t\tn = self.weights.shape[0] - 1\n\t\tfor x in range(n):\n\t\t\tself.weights[x, n] = p_g / n\n\t\t\tself.weights[n, x] = p_g / n\n\telse:\n\t\tself.universalGround = True\n\t\tn = self.weights.shape[0]\n\t\tnewW = np.zeros([n + 1, n + 1])\n\t\tnewW[:n, :n] = self.weights\n\t\tfor x in range(n):\n\t\t\tnewW[x, n] = p_g / n\n\t\t\tnewW[n, x] = p_g / n\n\t\tself.weights = newW\n\t\tself.addLandmark(Landmark(n, 0))\n\tself.normalizeWeights()\n\treturn self.weights\n</code></pre>"},{"location":"reference/#code.voltage.Problem.efficientSquareDistance","title":"<code>efficientSquareDistance(data)</code>","text":"<p>Computes the pairwise squared Euclidean distances of the rows in <code>data</code>.</p> <p>Uses the identity \u2016x\u2212y\u2016\u00b2 = \u2016x\u2016\u00b2 + \u2016y\u2016\u00b2 \u2212 2 x\u00b7y for efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Array of shape (n, d).</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>Matrix of shape (n, n) where entry (i, j) is squared distance.</p> Source code in <code>code\\voltage.py</code> <pre><code>def efficientSquareDistance(self, data: np.ndarray) -&gt; np.ndarray:\n\t\"\"\"\n\tComputes the pairwise squared Euclidean distances of the rows in `data`.\n\n\tUses the identity \u2016x\u2212y\u2016\u00b2 = \u2016x\u2016\u00b2 + \u2016y\u2016\u00b2 \u2212 2 x\u00b7y for efficiency.\n\n\tArgs:\n\t\tdata (ndarray): Array of shape (n, d).\n\n\tReturns:\n\t\tndarray: Matrix of shape (n, n) where entry (i, j) is squared distance.\n\t\"\"\"\n\tdata_norm2 = np.sum(data**2, axis=1)\n\tx_norm2 = data_norm2.reshape(-1, 1)\n\ty_norm2 = data_norm2.reshape(1, -1)\n\treturn x_norm2 + y_norm2 - 2 * data @ data.T\n</code></pre>"},{"location":"reference/#code.voltage.Problem.gaussiankernel","title":"<code>gaussiankernel(data, std)</code>","text":"<p>Builds a Gaussian (RBF) kernel matrix.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Array of shape (n, d).</p> required <code>std</code> <code>float</code> <p>Standard deviation parameter for the Gaussian.</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>Kernel matrix of shape (n, n).</p> Source code in <code>code\\voltage.py</code> <pre><code>def gaussiankernel(self, data: np.ndarray, std: float) -&gt; np.ndarray:\n\t\"\"\"\n\tBuilds a Gaussian (RBF) kernel matrix.\n\n\tArgs:\n\t\tdata (ndarray): Array of shape (n, d).\n\t\tstd (float): Standard deviation parameter for the Gaussian.\n\n\tReturns:\n\t\tndarray: Kernel matrix of shape (n, n).\n\t\"\"\"\n\tdist2 = self.efficientSquareDistance(data)\n\treturn np.exp(-dist2 / (2 * std**2))\n</code></pre>"},{"location":"reference/#code.voltage.Problem.normalizeWeights","title":"<code>normalizeWeights()</code>","text":"<p>Normalizes each row of the weight matrix to sum to 1.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any row sums to zero, resulting in NaNs.</p> Source code in <code>code\\voltage.py</code> <pre><code>def normalizeWeights(self) -&gt; None:\n\t\"\"\"\n\tNormalizes each row of the weight matrix to sum to 1.\n\n\tRaises:\n\t\tValueError: If any row sums to zero, resulting in NaNs.\n\t\"\"\"\n\tself.weights = self.weights / self.weights.sum(axis=1, keepdims=True)\n\tif np.isnan(self.weights).any():\n\t\traise ValueError(\"Array contains NaN values!\")\n</code></pre>"},{"location":"reference/#code.voltage.Problem.radialkernel","title":"<code>radialkernel(data, r)</code>","text":"<p>Builds a binary (0/1) radial kernel: 1 if distance \u2264 r, else 0.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Array of shape (n, d).</p> required <code>r</code> <code>float</code> <p>Radius threshold.</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>Adjacency-like matrix (n\u00d7n) of 0/1 floats.</p> Source code in <code>code\\voltage.py</code> <pre><code>def radialkernel(self, data: np.ndarray, r: float) -&gt; np.ndarray:\n\t\"\"\"\n\tBuilds a binary (0/1) radial kernel: 1 if distance \u2264 r, else 0.\n\n\tArgs:\n\t\tdata (ndarray): Array of shape (n, d).\n\t\tr (float): Radius threshold.\n\n\tReturns:\n\t\tndarray: Adjacency-like matrix (n\u00d7n) of 0/1 floats.\n\t\"\"\"\n\tdist2 = self.efficientSquareDistance(data)\n\treturn (dist2 &lt;= r**2).astype(float)\n</code></pre>"},{"location":"reference/#code.voltage.Problem.setKernel","title":"<code>setKernel(kernel)</code>","text":"<p>Sets the kernel function to use for weight computations.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>callable</code> <p>A function or callable object with signature                            kernel(X, Y, *params) \u2192 ndarray of shape (|X|, |Y|).</p> required Source code in <code>code\\voltage.py</code> <pre><code>def setKernel(self, kernel: Callable[..., np.ndarray]) -&gt; None:\n\t\"\"\"\n\tSets the kernel function to use for weight computations.\n\n\tArgs:\n\t\tkernel (callable): A function or callable object with signature\n\t\t\t\t\t\t   kernel(X, Y, *params) \u2192 ndarray of shape (|X|, |Y|).\n\t\"\"\"\n\tself.kernel = kernel\n</code></pre>"},{"location":"reference/#code.voltage.Problem.setPartitionWeights","title":"<code>setPartitionWeights(partition, *c)</code>","text":"<p>Computes and normalizes weights based on cluster centers and sizes.</p> <p>Parameters:</p> Name Type Description Default <code>partition</code> <code>Any</code> <p>An object with attributes <code>centers</code> (list of points)            and <code>point_counts</code> (counts per center).</p> required <code>*c</code> <code>Any</code> <p>Parameters to pass into the kernel function.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The normalized weight matrix for the partition block.</p> Source code in <code>code\\voltage.py</code> <pre><code>def setPartitionWeights(self, partition: Any, *c: Any) -&gt; np.ndarray:\n\t\"\"\"\n\tComputes and normalizes weights based on cluster centers and sizes.\n\n\tArgs:\n\t\tpartition: An object with attributes `centers` (list of points)\n\t\t\t\t   and `point_counts` (counts per center).\n\t\t*c: Parameters to pass into the kernel function.\n\n\tReturns:\n\t\tndarray: The normalized weight matrix for the partition block.\n\t\"\"\"\n\tcenters = np.array(partition.centers)\n\tcounts = np.array(partition.point_counts).reshape(-1, 1)\n\tK = self.kernel(centers[:, None], centers[None, :], *c)\n\tW = K * (counts @ counts.T)\n\tn = len(centers)\n\tself.weights[:n, :n] = W\n\tself.normalizeWeights()\n\treturn self.weights\n</code></pre>"},{"location":"reference/#code.voltage.Problem.setWeights","title":"<code>setWeights(*c)</code>","text":"<p>Computes and normalizes the weight matrix on the original data.</p> <p>Parameters:</p> Name Type Description Default <code>*c</code> <code>Any</code> <p>Parameters to pass into the currently set kernel function.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The normalized weight matrix (n\u00d7n).</p> Source code in <code>code\\voltage.py</code> <pre><code>def setWeights(self, *c: Any) -&gt; np.ndarray:\n\t\"\"\"\n\tComputes and normalizes the weight matrix on the original data.\n\n\tArgs:\n\t\t*c: Parameters to pass into the currently set kernel function.\n\n\tReturns:\n\t\tndarray: The normalized weight matrix (n\u00d7n).\n\t\"\"\"\n\tdata_np = self.data.getNumpy()\n\tn = len(self.data)\n\tself.weights[:n, :n] = self.kernel(data_np, *c)\n\tself.normalizeWeights()\n\treturn self.weights\n</code></pre>"},{"location":"reference/#code.voltage.Problem.timeEnd","title":"<code>timeEnd(replace=True)</code>","text":"<p>Computes the elapsed time since the last timeStart().</p> <p>Parameters:</p> Name Type Description Default <code>replace</code> <code>bool</code> <p>If True, resets the start time to now.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Seconds elapsed since last start.</p> Source code in <code>code\\voltage.py</code> <pre><code>def timeEnd(self, replace: bool = True) -&gt; float:\n\t\"\"\"\n\tComputes the elapsed time since the last timeStart().\n\n\tArgs:\n\t\treplace (bool): If True, resets the start time to now.\n\n\tReturns:\n\t\tfloat: Seconds elapsed since last start.\n\t\"\"\"\n\tcur_time = time.time()\n\tdiff = cur_time - self.start\n\tif replace:\n\t\tself.start = cur_time\n\treturn diff\n</code></pre>"},{"location":"reference/#code.voltage.Problem.timeStart","title":"<code>timeStart()</code>","text":"<p>Records the current time to measure elapsed intervals.</p> Source code in <code>code\\voltage.py</code> <pre><code>def timeStart(self) -&gt; None:\n\t\"\"\"\n\tRecords the current time to measure elapsed intervals.\n\t\"\"\"\n\tself.start = time.time()\n</code></pre>"},{"location":"reference/#code.voltage.Solver","title":"<code>Solver</code>","text":"<p>               Bases: <code>DistanceBased</code></p> <p>Solves a given Problem</p> Source code in <code>code\\voltage.py</code> <pre><code>class Solver(kmeans.DistanceBased):\n\t\"\"\"Solves a given Problem\"\"\"\n\tdef __init__(self, problem):\n\t\tself.problem = problem\n\t\tsuper().__init__()\n\n\tdef compute_voltages(self):\n\t\tn = self.problem.weights.shape[0]\n\n\t\tconstrained_nodes =   [l.index for l in self.problem.landmarks]\n\t\tunconstrained_nodes = [i for i in range(n) if i not in constrained_nodes]\n\n\t\tb = np.zeros(n)\n\t\tfor landmark in self.problem.landmarks:\n\t\t\tfor y in range(0, n):\n\t\t\t\tb[y] += landmark.voltage * self.problem.weights[y][landmark.index]\n\n\t\tA_unconstrained = np.identity(len(unconstrained_nodes)) - self.problem.weights[np.ix_(unconstrained_nodes, unconstrained_nodes)]\n\n\t\tb_unconstrained = b[unconstrained_nodes]\n\n\t\t# print(self.problem.weights)\n\t\t# print(A_unconstrained)\n\t\t# print(b_unconstrained)\n\n\t\tv_unconstrained = solve(A_unconstrained, b_unconstrained)\n\n\t\t# print(v_unconstrained)\n\n\t\tself.voltages = np.zeros(n)\n\n\t\tfor landmark in self.problem.landmarks:\n\t\t\tself.voltages[landmark.index] = landmark.voltage\n\n\t\tself.voltages[unconstrained_nodes] = v_unconstrained\n\n\t\tif (self.problem.universalGround):\n\t\t\tself.voltages = self.voltages[:-1]\n\n\t\treturn self.voltages\n\n\tdef approximate_voltages(self, epsilon=None, max_iters=None):\n\t\tn = self.problem.weights.shape[0]\n\n\t\tif (epsilon == None):\n\t\t\tif (max_iters == None):\n\t\t\t\tepsilon = 1 / n\n\n\t\tconstrained_nodes =\t\t[l.index for l in self.problem.landmarks]\n\t\tconstraints = \t\t\t[l.voltage for l in self.problem.landmarks]\n\t\tunconstrained_nodes =\t[i for i in range(n) if i not in constrained_nodes]\n\n\t\tself.voltages = np.zeros(n)\n\t\tvoltages = np.zeros(n)\n\n\t\tfor landmark in self.problem.landmarks:\n\t\t\tself.voltages[landmark.index] = landmark.voltage\n\n\t\tdist = self.distance(self.voltages, voltages)\n\t\tprev_dist = float('inf')\n\n\t\titerations = 0\n\n\t\twhile (((epsilon != None and dist &gt; epsilon * len(self.problem.data)) or (max_iters != None and iterations &lt; max_iters)) and dist &lt; prev_dist):\n\t\t\tvoltages = np.matmul(self.problem.weights, self.voltages)\n\t\t\tvoltages[constrained_nodes] = constraints\n\t\t\tprev_dist = dist\n\t\t\tdist = self.distance(self.voltages, voltages)\n\n\t\t\t# print(prev_dist, dist)\n\n\t\t\tself.voltages = voltages\n\t\t\titerations += 1\n\n\t\t# print(iterations)\n\n\t\tif (self.problem.universalGround):\n\t\t\tself.voltages = self.voltages[:-1]\n\n\t\treturn self.voltages\n\n\tdef localSolver(self, partitions, c):\n\t\tvoltages = [0 for i in range(len(self.problem.data))]\n\n\t\tfor index in range(partitions.k):\n\t\t\tclosestIndicies = partitions.getClosestPoints(index)\n\t\t\tcloseproblem.LandmarksIndicies = []\n\n\t\t\tfor pair in partitions.voronoi.ridge_points:\n\t\t\t\tif pair[0] == index:\n\t\t\t\t\tcloseproblem.LandmarksIndicies.append(pair[1])\n\t\t\t\tif pair[1] == index:\n\t\t\t\t\tcloseproblem.LandmarksIndicies.append(pair[0])\n\n\t\t\tcloseproblem.Landmarks = []\n\t\t\tfor cli in closeproblem.LandmarksIndicies:\n\t\t\t\tcloseproblem.Landmarks.append(Landmark(cli, self.voltages[cli]))\n\n\t\t\tlocalSolver = Solver(self.problem.data.getSubSet(closestIndicies))\n\t\t\tlocalSolver.setKernel(self.problem.gaussiankernel)\n\t\t\tlocalSolver.setWeights(c)\n\t\t\tlocalSolver.addproblem.Landmarks(closeproblem.Landmarks)\n\t\t\tlocalVoltages = localSolver.compute_voltages()\n\n\t\t\tfor i, v in zip(closestIndicies, localVoltages):\n\t\t\t\tvoltages[i] = v\n\n\t\treturn voltages\n</code></pre>"}]}